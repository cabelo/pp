{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_vqFo-wFgJn"
   },
   "source": [
    "# PPD: MPI e programação com passagem de mensagem\n",
    "\n",
    "Hélio - DC/UFSCar - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqLeez1BbscH"
   },
   "source": [
    "# Sobre MPI\n",
    "\n",
    "[MPI](https://www.mpi-forum.org) é uma biblioteca para programação paralela distribuída, comumente usada para a criação de aplicações que se espalham por diferentes computadores interligados em rede. \n",
    "\n",
    "Os recursos de MPI permitem não só **iniciar** os processos em nós distintos, mas também oferecem formas de **identificar** logicamente os processos, o que simplifica muito as comunicações entre eles. Além disso, a biblioteca trata do **empacotamento** e **desempacotamento** dos dados em *buffers* para as transmissões, o que evita que o programador tenha que preocupar-se com conversões de formato e com a preservação dos significados dos dados transmitidos.\n",
    "\n",
    "O modelo de aplicação é comumente **SPMD**, em que um nó (processo) mestre executa funções de coordenação e controle e os demais nós são trabalhadores, que usualmente replicam suas atividades sobre partes disjuntas dos dados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfr-Oqpwvbjm"
   },
   "source": [
    "# Estrutura dos programas\n",
    "\n",
    "Para usar a estrutura MPI, é preciso fazer uma chamada à função **MPI_Init**, o que deve ocorrer antes de qualquer outra chamada a funções da biblioteca.\n",
    "\n",
    "Tipicamente, os mesmos parâmetros que foram recebidos pela função *main* (argc e argv) são repassados à função de ativação da biblioteca. \n",
    "\n",
    "Cabe à função [MPI_Init](https://www.open-mpi.org/doc/v3.1/man3/MPI_Init.3.php) executar as inicializações locais ao programa, como eventuais alocações de *buffers* internos, além de realizar comunicações com os demais nós lançados na ativação da aplicação. Tudo isso, contudo, é feito de forma transparente para a aplicação que invoca a chamada.\n",
    "\n",
    "Ao final do programa, é preciso executar a função [MPI_Finalize](https://www.open-mpi.org/doc/v4.0/man3/MPI_Finalize.3.php), que encerra as operações da biblioteca. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xifmGPCQ_33F"
   },
   "source": [
    "\n",
    "# Grupo de processos\n",
    "\n",
    "Uma aplicação MPI é comumente composta por um **grupo de processos**, epecificados na ativação do programa. \n",
    "\n",
    "Vários sub-grupos podem ser formados, de acordo com a lógica da aplicação, mas há ao menos um grupo padrão, chamado **MPI_COMM_WORLD**. Esse grupo inclui todos os processos da aplicação e, dentro desse grupo, cada processo é identificado por um **número lógico**, chamado ***rank***, que varia de 0 a N-1. O índice 0 é atribuído ao processo que iniciou a aplicação. \n",
    "\n",
    "Um aspecto interessante desse conceito de grupos e da identificação lógica dos processos é que a aplicação não precisa preocupar-se com os endereços (IP, e.g) dos nós em que os processos foram iniciados, ou mesmo com detalhes do protocolo da camada de transporte e da identificação dos números de portas associados a *sockets*. \n",
    "\n",
    "Como veremos, nas operações de transmissão da API MPI, basta identificar os processos de acordo com seus números lógicos, cabendo à biblioteca cuidar de todos os detalhes para que as comunicações ocorram entre os processos apropriados.\n",
    "\n",
    "Tremenda mão na roda, não é?!\n",
    "\n",
    "Uma vez compilado, um programa MPI pode ser ativado com números variados de processos; ou seja, pode-se experimentar usar mais ou menos processos em cada execução, sem que seja preciso recompilar o programa. \n",
    "\n",
    "Para que a aplicação saiba em tempo de execução quantos processos estão sendo usados nesta execução, a biblioteca oferece funções que retornam este número. Há também uma função para que cada processo saiba qual é seu índice lógico dentro do grupo MPI_COMM_WORLD (e de qualquer outro sub-grupo).\n",
    "\n",
    "Esses serviços são providos pelas funções [MPI_Comm_size](https://www.open-mpi.org/doc/v3.1/man3/MPI_Comm_size.3.php)() e [MPI_Comm_rank](https://www.open-mpi.org/doc/v4.0/man3/MPI_Comm_rank.3.php)().\n",
    "\n",
    "Vejamos um exemplo de código MPI a seguir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erpktu240L3l",
    "outputId": "3c8cab09-cde1-4dcb-baa0-196bc17a0efa"
   },
   "outputs": [],
   "source": [
    "%%writefile m1.c\n",
    "\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint numtasks, rank, status, namelen;\n",
    "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "\n",
    "\t// função obrigatória, usada para inicialização das atividades relacionadas a MPI\n",
    "\tstatus = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (status != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro em MPI_Init. Terminando...\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, status);\n",
    "\t} \n",
    "\t\n",
    "  // obtém o número de processos sendo usados nesta execução\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    " \n",
    "  // obtém o rank deste processo em relação aos processos da aplicação\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "\t// int MPI_Get_processor_name(char *name, int *resultlen);\n",
    "\t// Returns the name of the processor on which it was called.\n",
    "\tMPI_Get_processor_name(processor_name, &namelen);\n",
    "\n",
    "\tprintf(\"Processo %d de %d em %s\\n\", rank,numtasks,processor_name);\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-wIk7fEBnCe"
   },
   "source": [
    "# Compilação de programas MPI\n",
    "\n",
    "As extensões providas para uso de MPI incluem suporte para as linguagens C/C++ e Fortran. \n",
    "\n",
    "Tomando como referência a programação em C, a compilação de programas envolve os seguintes aspectos:\n",
    "\n",
    "* inclusão do cabeçalho <mpi.h> contendo as definições de tipos e os protótipos das funções da biblioteca;\n",
    "* especificação da localização desta bibliteca para a compilação do programa;\n",
    "* especificação da localização das bibliotecas com os códigos compilados das funções MPI para a fase de \"ligação\" (*link*) do programa;\n",
    "* indicação para ligação (*link*) do código das biblioteas na fase de geração de código, ou ajuste para carregamento da bibliteca em tempo de execução, no caso de ligação dinâmica.\n",
    "\n",
    "Para simplificar a compilação dos programas, há algunsn *scripts* utilitários que são instalados junto com a aplicação, como o comando [mpicc](https://www.open-mpi.org/doc/v4.0/man1/mpicc.1.php). \n",
    "\n",
    "De maneira geral, usa-se este comando com a mesma sintaxe das chamadas a gcc.\n",
    "\n",
    "```\n",
    "$ mpicc prog.c -o prog\n",
    "```\n",
    "\n",
    "Vejamos como compilar o arquivo de programa apresentado anteriormente (m1.c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtWPp5i-DumI",
    "outputId": "458544bf-8b69-4a97-d573-05c9ac067759"
   },
   "outputs": [],
   "source": [
    "# Vamos verificar se MPI está instalado e se o utlitário mpicc está acessível\n",
    "! whereis mpicc\n",
    "\n",
    "# Compilemos o programa -- DEVCLOUD\n",
    "!chmod 755 compile.sh; !chmod 755 q; ./q compile.sh m1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yzp51QWPaS4m"
   },
   "source": [
    "# Ativação de programas MPI\n",
    "\n",
    "A ativação de um programa mpi é comumente feita com os comandos **[mpirun](https://www.open-mpi.org/doc/current/man1/mpirun.1.php)**, ***mpiexec*** e ***orterun***, para a implementação **OpenMPI**.\n",
    "\n",
    "A ativação de programas pode ocorrer nos modelos **SPMD** (*single program multiple data*) e **MPMD** (*multiple program multiple data*).\n",
    "\n",
    "* **SPMD**: o mesmo programa (arquivo executável) é ativado em todos os nós. Cabe à lógica do programa diferenciar cada instância e fazer com que trechos de código específicos, como uma função mestre e outra trabalhador, dentro do mesmo programa, sejam executadas nos nós. \\\n",
    "\n",
    "      $ mpirun [opções] programa\n",
    "\n",
    "\n",
    "* **MPMD**: programas distintos podem ser especificados para nós ou grupoos de nós. \\\n",
    "\n",
    "      $ mpirun [ opções globais ] [ opções locais 1] ... : [opções locais 2]  \n",
    "      // : é usado para separar cada programa e hosts que o executarão\n",
    "      // O parâmetro -np, ou -n, indica o número de processos que serão usados. \n",
    "\n",
    "Já os *hosts* a serem utilizados podem ser especificados individualmente na linha de comando, ou podem estar listados num arquivo texto especificado.\n",
    "\n",
    "* Especificação dos hosts:\n",
    "      $ mpirun -n 3 -host h1,h2,h3  prog ...\n",
    "\n",
    "* Especificação de arquivo de *hosts*: \n",
    "    \n",
    "      $ mpirun -hostfile arq_hosts  prog ...\n",
    "  ***ou*** \n",
    "      $ mpirun --machinefile arq_hosts prog ...\n",
    "\n",
    "Exemplos:\n",
    "```\n",
    "$ mpirun prog \n",
    "$ mpirun -np 4 prog                                 // executa o programa com 4 processos\n",
    "$ mpirun -np 4 -host serv1,serv2,serv3,serv3 prog   // distribui 4 instâncias do programa, sendo 2 em serv3\n",
    "$ mpirun -np 2 prog1 : -np 4 prog2                  // cria programa com 2 instâncias de prog1 e 4 de prog2\n",
    "$ mpirun -np 4 -hostfile hosts prog                 // especifica o arquivo com a configuração dos hosts\n",
    "$ mpirun -n 10 -host serv1:2,serv2:4,serv3:4\n",
    "```\n",
    "\n",
    "Uma vez indicados os *hosts*, a atribuição de processos a esses nós pode ocorrer segundo 2 políticas: ***by slot*** e ***by node***.\n",
    "\n",
    "* ***By slot***: política padrão, na qual MPI atribui processos aos nós até completarem-se os *slots* disponíveis.\n",
    "* ***By node***: selecionada com o parâmetro *--bynode* na ativação do programa, essa política faz com que MPI atribua um processo a cada nó de cada vez, de forma circular entre os nós com *slots* disponíveis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiqfvZ2C6MqV"
   },
   "source": [
    "O exemplo a seguir gera um arquivo de hosts (*hostfile*). Observe que diferentes opções são comentadas, mas apenas a indicação de que o host local está disponível com 4 *slots* será considerada. As demais opções estão comentadas no arquivo, usando o caractere # no início da linha.\n",
    "\n",
    "Vale saber que, com TCP/IP, o nome DNS [*localhost*](https://en.wikipedia.org/wiki/Localhost) sempre se refere ao próprio computador e está associado ao endereço 127.0.0.1, que é um endereço reservado e associado à interface *loopback*, local a todo sistema. Ou seja, qualquer comunicação com o nome *localhost* acabará sendo tratada pelo sistema local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5oKx93wVzip",
    "outputId": "01c02238-c01e-4ab4-efe6-03811aa14467"
   },
   "outputs": [],
   "source": [
    "%%writefile hosts\n",
    "\n",
    "localhost slots=4\n",
    "\n",
    "# O arquivo de hosts é um arquivo texto, contendo, em cada linha, a especificação \n",
    "# de um computador onde é possível iniciar a execução das aplicações.\n",
    "#\n",
    "# no1.domain.com\n",
    "# no2.domain.com\n",
    "\n",
    "# A ordem dos hosts listados pode não ser seguida na ativação do programa.\n",
    "# Para cada host, é possível especificar também o número de \"slots\", que representa\n",
    "# o número de instâncias (processos) que podem ser ativadas neste nó. \n",
    "# Normalmente, esse número é relacionado ao número de processadores disponíveis no nó.\n",
    "#\n",
    "# no1 slots=2\n",
    "# no2 slots=2\n",
    "# no3 slots=4\n",
    "\n",
    "# O número máximo de slots também pode ser especificado e indica o número  \n",
    "# máximo de processos que podem ser atribuídos ao nó.\n",
    "#\n",
    "# no1 slots=4 max-slots=4\n",
    " \n",
    "# Uma vez definidos os computadores disponíveis no arquivo de hosts, a indicação \n",
    "# de quais computadores usar pode ser feita especificando o nome do arquivo na \n",
    "# ativação do programa\n",
    "#\n",
    "# $ mpirun -np 4 --hostfile hosts prog \n",
    "# ou\n",
    "# $ mpiru -np 4 --machinefile hosts prog \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiNEZRWQ6-Y_"
   },
   "source": [
    "O exemplo de execução a seguir tem alguns comandos que permitem obter diferentes informações sobre o *hostname* e sobre a versão MPI disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aI7gBVC2Cuxx",
    "outputId": "72eca206-0584-4139-fab2-ee5990991deb"
   },
   "outputs": [],
   "source": [
    "! lscpu | grep CPU && echo\n",
    "! echo \"hostname: \" $(hostname) && echo\n",
    "! which mpicc mpirun orterun && echo\n",
    "! mpirun --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBg4ty228F-g"
   },
   "source": [
    "Os comandos a seguir realizam a compilação do programa m1.c, apresentado anteriormente, e o exeutam usando 4 instâncias de processos no host local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anHrJO9ME8Hj"
   },
   "source": [
    "Vejam que mesmo o programa estando compilado, é possível iniciar sua execução usando diferentes números e conjuntos de computadores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ7NIMFiE6zd"
   },
   "outputs": [],
   "source": [
    "!chmod  755 launch.sh; chmod 755 q; ./q launch.sh m1 nodes=2:ppn=2;\n",
    "# Parametros para Scripts: \n",
    "# m1 -> binario de execucao \n",
    "# nodes=2:ppn=2 -> total de nodes=2 && processos por node=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx9kN-9H-Wau"
   },
   "source": [
    "# Criando programas MPI: modelo de processos\n",
    "\n",
    "Na programação paralela distribuída, ou seja, usando passagem de mensagens para as comunicações entre as tarefas, é comum usar-se um modelo **mestre / trabalhador** (*master / worker*).\n",
    "\n",
    "Em geral, cabe ao processo **mestre** **enviar os dados** para os processos trabalhadores, que irão manipulá-los, e aguardar os resultados. Já nos processos que estiverem fazendo o papel de **trabalhador**, é preciso esperar pelos dados, processá-los localmente e enviar os resultados de volta ao servidor.\n",
    "\n",
    "Esse modelo é comum quando os dados a manipular estão em arquivo, acessível por um nó apenas. Cabe então ao processo nesse nó ler os dados e repassá-los, **em partes**, aos demais.\n",
    "\n",
    "    Obs 1: Um aspecto importante a observar aqui é que estamos tratando de **processos**, \n",
    "    já que trata-se de uma aplicação que envolve atividades em diferentes nós. Ou seja, \n",
    "    diferentes sistemas computacionais conectados em rede. É claro que cada processo\n",
    "    poderá ter várias threads, mas trataremos disso posteriormente...\n",
    "\n",
    "    Obs 2: É claro que é possível ter um sistema de arquivos distribuídos, \n",
    "    de forma que cada processo leia os dados localmente. Isso é feito em \n",
    "    alguns cenários, como com MPI_IO, por exemplo. \n",
    "\n",
    "```\n",
    "    Master      W1      W2     W3     W4   ...  Wn\n",
    "      |          |      |      |      |         |\n",
    "      | -------->|      |      |      |   ...   |   W1 recebe e começa processar\n",
    "      | ---------|----->|      |      |   ...   |   W2 recebe e começa ...\n",
    "      | ---------|------|----->|      |         |    ...\n",
    "      | ---------|------|------|----->|         |    ...\n",
    "      |   ...    |      |      |      |         |    ...\n",
    "      | ---------|------|------|------|-------->|   Wn recebe e começa ...\n",
    "      |    .     |      |      |      |         |\n",
    "      |    .     |      |      |      |         |\n",
    "      |    .     |      |      |      |         |\n",
    "      |    .     |      |      |      |         |\n",
    "      |<---------|      |      |      |   ...   |  W1 conclui e envia resultados\n",
    "      |<---------|------|      |      |   ...   |  W2 envia resultados\n",
    "      |<---------|------|------|      |         |   ...\n",
    "      |<---------|------|------|------|         |   ...\n",
    "      |   ...    |      |      |      |         |\n",
    "      |<---------|------|------|------|---------|  Wn envia resulados\n",
    "      |          |      |      |      |         |\n",
    "\n",
    "```\n",
    "\n",
    "Há situações em que há várias \"rodadas\" dessa forma de interação no programa. Há casos também de processamento contínuo, em que o servidor e os clientes ficam permanentemente nesse ciclo de envio e recebimento.\n",
    "\n",
    "Também é possível usar um modelo de ***pipeline***, em que cada processo faz uma manipulação nos dados de entrada e repassa os dados processados para um próximo nó. Esse próximo nó repassa os dados para o próximo e aguarda novos dados. Tudo isso num ciclo.\n",
    "\n",
    "```\n",
    "    Master      W1      W2     W3     W4   ...  Wn\n",
    "      |         |      |      |      |         |\n",
    "      |-------->|      |      |      |   ...   |  W1 recebe e começa processar\n",
    "      |         |      |      |      |         |\n",
    "      |         |      |      |      |         |\n",
    "      |         |----->|      |      |   ...   |  W2 recebe e começa ...\n",
    "      |-------->|      |      |      |         |\n",
    "      |         |      |      |      |         |\n",
    "      |         |      |----->|      |         |  W3 recebe e começa ...\n",
    "      |         |----->|      |      |         |\n",
    "      |-------->|      |      |      |    ...\n",
    "      |         |      |      |----->|         |\n",
    "      |         |      |----->|      |         |\n",
    "      |         |----->|      |      |         |\n",
    "      |-------->|      |      |      |         |\n",
    "      |         |      |      |      |         |\n",
    "      |         |      |      |      |---...-->|  Wn recebe e começa... \n",
    "      |   ...   |      |      |----->|         |  ... pipeline entra em    \n",
    "      |         |      |----->|      |         |      regime de produção,\n",
    "      |         |----->|      |      |         |      com todos os nós \n",
    "      |-------->|      |      |      |         |      trabalhando!\n",
    "      |         |      |      |      |         | \n",
    "      |   ...   |      |      |      |         | \n",
    "```\n",
    "\n",
    "Como veremos posteriormente, MPI também possui primitivas para comunicação coletiva, que permitem o envio de mensages para todos os processos em um grupo.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESysSnAVSo_p"
   },
   "source": [
    "# Criando programa MPI: SMPD x MPMD\n",
    "\n",
    "Seja qual for o modelo de processos adotado na programação, *master/worker*, *pipeline*, ou outro, cabe ainda ao programador decidir se ele vai criar todo o código no mesmo programa ou se vai usar programas separados para cada papel.\n",
    "\n",
    "Nos casos mais comuns, a criação de programas com MPI é feita no modelo SPMD. Isso quer dizer que o programador cria um único programa, que vai ser iniciado em todos os nós de processamento. \n",
    "\n",
    "Assim, é usual criar os 2 (ou mais) papéis dentro do mesmo código e usar o ***rank*** do processo para determinar qual parte do código será executada.\n",
    "\n",
    "Neste caso, compila-se este programa e pode-se submetê-lo à execução usando diferentes números de processos e nós de processamento.\n",
    "\n",
    "No início do código, é comum que o programa contenha chamadas às funções MPI para determinar quantos processos estão sendo usados nesta execução e dividir as atividades de maneira apropriada. Tem-se, assim um programa que pode adequar-se à escalabilidade do sistema computacional.\n",
    "\n",
    "Desta forma, fica bem mais fácil para o programador experimentar executar o programa com números variados de processos e determinar qual é a configuração que resulta em melhor desempenho.\n",
    "\n",
    "## Iniciando programas MPI no modelo SMPD\n",
    "\n",
    "Usando um código só, que define a função de cada processo internamente, a ativação de um programa MPI fica pareceida com algo assim:\n",
    "\n",
    "```\n",
    "$ mpirun prog -n XXX -hostfile HHH ...\n",
    "```\n",
    "\n",
    "Já se forem criadas versões separadas para o código do *mestre* e o código dos *trabalhadores*, a linha de comando fica parecida com:\n",
    "\n",
    "```\n",
    "$ mpirun master  -n XXX  worker  -hostfile HHH ... \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obAilwScyP6f",
    "outputId": "66190bcc-c569-42f1-d1ac-52b115ad6e04"
   },
   "outputs": [],
   "source": [
    "%%writefile m-w.c\n",
    "\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint numtasks, rank, status;\n",
    "\n",
    "\tstatus = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (status != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro em MPI_Init. Terminando...\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, status);\n",
    "\t}\n",
    "  // obtém o número de processos sendo usados nesta execução\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    " \n",
    "  // obtém o rank deste processo em relação aos processos da aplicação\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "  if(rank==0) {\n",
    "    printf(\"Master (%d / %d)\\n\",rank,numtasks);\n",
    "  } else {\n",
    "     printf(\"Worker (%d / %d)\\n\",rank,numtasks);\n",
    "  }\n",
    "\n",
    "\t// talvez devesse esperar os processos filhos terminarem antes de terminar...\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dytBm9vhzHq-",
    "outputId": "4aabfaf7-ed66-4144-cdf4-238d2a630eb0"
   },
   "outputs": [],
   "source": [
    "%%writefile master.c\n",
    "\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint numtasks, rank, status;\n",
    "\n",
    "\tstatus = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (status != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro em MPI_Init. Terminando...\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, status);\n",
    "\t}\n",
    "  // obtém o número de processos sendo usados nesta execução\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    " \n",
    "  // obtém o rank deste processo em relação aos processos da aplicação\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "  // Código executado só pelo processo master\n",
    "  printf(\"Master (%d / %d)\\n\",rank,numtasks);\n",
    " \n",
    "\t// Na prática, deveria esperar os demais processos terminarem antes de encerrar a aplicação...\n",
    "\tsleep(3);\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrNQSNIDzQ5V",
    "outputId": "3779e075-c75a-4037-9f2d-17942ebd5c04"
   },
   "outputs": [],
   "source": [
    "%%writefile worker.c\n",
    "\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int length;\n",
    "char hostname[MPI_MAX_PROCESSOR_NAME];\n",
    "\n",
    "int\n",
    "main( int argc, char *argv[])\n",
    "{\n",
    "\tint numtasks, rank, status;\n",
    "\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "\n",
    "\tstatus = MPI_Init(&argc,&argv);\n",
    "\n",
    "\tif (status != MPI_SUCCESS) {\n",
    "\t\tprintf (\"Erro em MPI_Init. Terminando...\\n\");\n",
    "\t\tMPI_Abort(MPI_COMM_WORLD, status);\n",
    "\t}\n",
    "  // obtém o número de processos sendo usados nesta execução\n",
    "\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n",
    " \n",
    "  //obtem o nome do host em execucao\n",
    "    MPI_Get_processor_name(hostname, &length);\n",
    "\n",
    "  // obtém o rank deste processo em relação aos processos da aplicação\n",
    "\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n",
    "\n",
    "  // Código executado só pelos processos workers\n",
    "  printf(\"Worker (%d / %d) \\n\",rank,numtasks);\n",
    "\n",
    "\tMPI_Finalize();\n",
    "\n",
    "\treturn(0);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvWh_19Kzibl",
    "outputId": "002c9251-9853-459e-a209-48b5484ed028"
   },
   "outputs": [],
   "source": [
    "! echo \"Executando um programa no modelo SPMD (mesmo programa no master e nos workers)\"\n",
    "! if [ ! m-w -nt m-w.c ]; then mpirun -np 5 ./master : -np 4 ./worker ; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Executando um programa no modelo MPMD (programas distintos no master e nos workers)\"\n",
    "! if [ ! master -nt master.c ]; then ./compile.sh master; fi\n",
    "! if [ ! worker -nt worker.c ]; then ./compile.sh worker; fi\n",
    "! mpirun -np 1 ./master : -np 4 -host localhost:5 ./worker"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Intel® oneAPI 2023.0)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
