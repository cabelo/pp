{"cells":[{"cell_type":"markdown","metadata":{"id":"QOXlGylgV9Vv"},"source":["# Uso efetivo da Memória do Subsistema"]},{"cell_type":"markdown","metadata":{"id":"AMDmktziV9Vz"},"source":["Agora que você pode escrever os kernels CUDA corretos e entender a importância de lançar *grids* que dão à GPU oportunidade suficiente para ocultar a latência, você aprenderá técnicas para utilizar efetivamente os subsistemas de memória da GPU. Essas técnicas são amplamente aplicáveis a uma variedade de aplicativos CUDA e algumas das mais importantes quando se trata de tornar seu código CUDA mais rápido.\n","\n","Você vai começar aprendendo sobre união de memória(coalescing memory). Para desafiar sua capacidade de raciocinar sobre união de memória(coalescing memory) e expor detalhes importantes relevantes para muitos aplicativos CUDA, você aprenderá sobre *grids* bidimensionais e blocos de thread. Em seguida, você aprenderá sobre um espaço de memória sob demanda muito rápido, controlado pelo usuário, chamado memória compartilhada, e usará a memória compartilhada para facilitar a fusão de memória onde de outra forma não seria possível. Por fim, você aprenderá sobre conflitos de banco de memória compartilhada, que podem prejudicar as possibilidades de desempenho do uso de memória compartilhada e uma técnica para resolvê-los."]},{"cell_type":"markdown","metadata":{"id":"-cC7dArgV9V0"},"source":["## Objetivos"]},{"cell_type":"markdown","metadata":{"id":"BXyjcyMcV9V1"},"source":["Ao concluir esta seção, você será capaz de:\n","* Escrever kernels CUDA que se beneficiam de padrões de acesso à memória aglutinados.\n","* Trabalhar com grids multidimensionais e blocos de thread.\n","* Usar memória compartilhada para coordenar threads dentro de um bloco.\n","* Usar a memória compartilhada para facilitar os padrões de acesso à memória aglutinada.\n","* Resolva conflitos de banco de memória compartilhada."]},{"cell_type":"markdown","metadata":{"id":"bBvEG-nsV9V1"},"source":["## O problema: o acesso à memória não consolidada prejudica o desempenho\n","Antes de aprender os detalhes sobre o que é **acesso à memória combinada**, execute as células a seguir para observar as implicações de desempenho de uma alteração aparentemente trivial no padrão de acesso a dados em um kernel."]},{"cell_type":"markdown","metadata":{"id":"PDpjn2sOV9V2"},"source":["### Criação de dados\n","Nesta célula definimos `n` e criamos uma grid com threads iguais a `n`. Também criamos um vetor de saída com comprimento `n`. Para as entradas criamos vetores de tamanho `stride * n` por razões que serão esclarecidas abaixo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YT-hfMqKV9V2"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","n = 1024*1024 # 1M\n","\n","threads_per_block = 1024\n","blocks = int(n / threads_per_block)\n","\n","stride = 16\n","\n","# Input Vectors of length stride * n\n","a = np.ones(stride * n).astype(np.float32)\n","b = a.copy().astype(np.float32)\n","\n","# Output Vector\n","out = np.zeros(n).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_b = cuda.to_device(b)\n","d_out = cuda.to_device(out)\n"]},{"cell_type":"markdown","metadata":{"id":"dEd9PLg1V9V4"},"source":["### Definição do Kernel\n","Em `add_experiment`, cada thread na grid adicionará um item em `a` e um item em `b` e escreverá o resultado em `out`. O kernel foi escrito de forma que podemos passar um valor `coalesced` de `True` ou `False` para afetar como ele indexa nos vetores `a` e `b`. Você verá a comparação de desempenho dos dois modos abaixo.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBx-vMdhV9V5"},"outputs":[],"source":["@cuda.jit\n","def add_experiment(a, b, out, stride, coalesced):\n","    i = cuda.grid(1)\n","    # The above line is equivalent to\n","    # i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n","    if coalesced == True:\n","        out[i] = a[i] + b[i]\n","    else:\n","        out[i] = a[stride*i] + b[stride*i]"]},{"cell_type":"markdown","metadata":{"id":"1khWuy-1V9V5"},"source":["### Lançamento do Kernel Usando Acesso Agrupado (Coalesced Access)\n","Aqui passamos `True` como o valor `coalesced` e observamos o desempenho do kernel em várias execuções:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAKzOlItV9V6"},"outputs":[],"source":["%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, True); cuda.synchronize()"]},{"cell_type":"markdown","metadata":{"id":"O91H5OHsV9V6"},"source":["Aqui, garantimos que o kernel funcionou conforme o esperado:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhmdMnhrV9V7"},"outputs":[],"source":["result = d_out.copy_to_host()\n","truth = a[:n] + b[:n]\n","np.array_equal(result, truth)"]},{"cell_type":"markdown","metadata":{"id":"1IMJuLbqV9V7"},"source":["### Resultados\n","\n",">Troque o parâmetro para `False` e execute novamente. \n","\n","O desempenho do padrão de acesso a dados não agrupados(Uncoalesced) foi muito pior. Agora você aprenderá por que e como pensar sobre os padrões de acesso a dados em seus kernels para obter kernels de alto desempenho."]},{"cell_type":"markdown","metadata":{"id":"yN81qDOcV9V7"},"source":["## Apresentação: Memória Global Aglutinada\n","\n","> **Nota de rodapé**: para obter detalhes adicionais sobre o tamanho do segmento de memória global em vários dispositivos e com relação ao armazenamento em cache, consulte o [Guia de práticas recomendadas de CUDA](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory).\n"]},{"cell_type":"code","source":["from IPython.display import IFrame\n","IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/coalescing-v3.pptx', 800, 450)"],"metadata":{"id":"0VJxaNMeWGDa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> _**Nota**: para detalhes adicionais sobre tamanhos de segmentos de memória global em diferentes dispositivos, e sobre memória cache, consulte [The CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory)._"],"metadata":{"id":"BVaL8TBYYeKq"}},{"cell_type":"markdown","metadata":{"id":"E1oKxSC9V9V8"},"source":["## Exercício: Somas de Colunas e Linhas\n","\n","Para este exercício, você deverá escrever um kernel de somas de colunas que use padrões de acesso à memória totalmente agrupados(coalesced memory access). Para começar, você observará o desempenho de um kernel de soma de linhas que faz acessos à memória não agrupados(uncoalesced memory access).\n","\n","### Somas de linha"]},{"cell_type":"markdown","metadata":{"id":"MXQttOGxV9V9"},"source":["**Criação de Dados**\n","Nesta célula criamos uma matriz de entrada, bem como um vetor para armazenar a solução, e transferimos cada um deles para o dispositivo. Também definimos as dimensões da grid e do bloco a serem usadas quando lançamos o kernel abaixo. Definimos uma linha arbitrária de dados para algum valor arbitrário para facilitar a verificação de correção abaixo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5g2WtqtV9V9"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","n = 16384 # matrix side size\n","threads_per_block = 256\n","blocks = int(n / threads_per_block)\n","\n","# Input Matrix\n","a = np.ones(n*n).reshape(n, n).astype(np.float32)\n","# Here we set an arbitrary row to an arbitrary value to facilitate a check for correctness below.\n","a[3] = 9\n","\n","# Output vector\n","sums = np.zeros(n).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_sums = cuda.to_device(sums)"]},{"cell_type":"markdown","metadata":{"id":"yDCPcrHaV9V-"},"source":["**Definição do Kernel** :\n","`row_sums` usará cada thread para iterar sobre uma linha de dados, somando ela e, em seguida, armazenará a soma da linha em `sums`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix_9ApDvV9V-"},"outputs":[],"source":["@cuda.jit\n","def row_sums(a, sums, n):\n","    idx = cuda.grid(1)\n","    sum = 0.0\n","    \n","    for i in range(n):\n","        # Each thread will sum a row of `a`\n","        sum += a[idx][i]\n","        \n","    sums[idx] = sum"]},{"cell_type":"markdown","metadata":{"id":"guKuQ3WKV9V_"},"source":["**Performance da Soma de Linha e Checagem de Corretude**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8viKFkIdV9V_"},"outputs":[],"source":["%timeit row_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"]},{"cell_type":"code","source":["result = d_sums.copy_to_host()\n","truth = a.sum(axis=1)\n","np.array_equal(truth, result)"],"metadata":{"id":"YpG6KAm8cM6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jW3lRFypV9V_"},"source":["### Soma de Colunas"]},{"cell_type":"markdown","metadata":{"id":"wEm3XbwkV9WA"},"source":["**Criação de Dados**: Nesta célula criamos uma matriz de entrada, bem como um vetor para armazenar a solução, e transferimos cada um deles para o dispositivo. Também definimos as dimensões da grid e do bloco a serem usadas quando lançamos o kernel abaixo. Definimos uma coluna arbitrária de dados para algum valor arbitrário para facilitar a verificação de correção abaixo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOol_J4_V9WA"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","n = 16384 # matrix side size\n","threads_per_block = 256\n","blocks = int(n / threads_per_block)\n","\n","a = np.ones(n*n).reshape(n, n).astype(np.float32)\n","# Here we set an arbitrary column to an arbitrary value to facilitate a check for correctness below.\n","a[:, 3] = 9\n","sums = np.zeros(n).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_sums = cuda.to_device(sums)\n"]},{"cell_type":"markdown","metadata":{"id":"WUl_DgkBV9WA"},"source":["**Definição do Kernel** : `col_sums` usará cada thread para iterar sobre uma coluna de dados, somando-os e, em seguida, armazenará a soma da coluna em `sums`. Conclua a definição do kernel para fazer isso. Como exercício, faça este exemplo abaixo. \n","\n","Caso esteja com dúvidas, verifique a solução em algumas células abaixo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-5iFGiZV9WB"},"outputs":[],"source":["@cuda.jit\n","def col_sums(a, sums, ds):\n","    # TODO: Write this kernel to store the sum of each column in matrix `a` to the `sums` vector.\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"qJUES5fPV9WB"},"source":["### **Solução**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ge6eV6yPV9WC"},"outputs":[],"source":["@cuda.jit\n","def col_sums(a, sums, ds):\n","  idx = cuda.grid(1)\n","  sum = 0.0\n","  \n","  for i in range(ds):\n","    # Cada thread soma uma coluna. \n","    sum += a[i][idx]\n","      \n","  sums[idx] = sum"]},{"cell_type":"markdown","metadata":{"id":"OzuifLk1V9WC"},"source":["**Verifique o desempenho**: supondo que você tenha escrito `col_sums` para usar padrões de acesso agrupados, você deve ver uma velocidade significativa (quase 2x) em comparação com os `row_sums` não agrupados executados acima:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLbNoj_FV9WC"},"outputs":[],"source":["%timeit col_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KkmYc2yV9WD"},"outputs":[],"source":["result = d_sums.copy_to_host()\n","truth = a.sum(axis=0)\n","np.array_equal(truth, result)"]},{"cell_type":"markdown","metadata":{"id":"OkG77KdfV9WD"},"source":["## Blocos e grid's de 2 ou 3 dimensões\n","\n","Tanto as grid's quanto os blocos podem ser configurados para conter uma coleção de blocos ou threads de 2 ou 3 dimensões, respectivamente. Isso é feito principalmente por uma questão de conveniência para programadores que geralmente trabalham com conjuntos de dados bidimensionais ou tridimensionais. Aqui está um exemplo muito trivial para destacar a sintaxe. Você pode precisar ler *ambos*, a definição do kernel e seu lançamento antes que o conceito faça sentido."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Vl2TqciV9WE"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","A = np.zeros((4,4)) # A 4x4 Matrix of 0's\n","d_A = cuda.to_device(A)\n","\n","# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n","# by using a Python tuple to signify grid and block dimensions.\n","blocks = (2, 2)\n","threads_per_block = (2, 2)"]},{"cell_type":"markdown","metadata":{"id":"2Sx3iO2zV9WE"},"source":["Este kernel irá pegar uma matriz de entrada de 0's e escrever para cada um de seus elementos, suas coordenadas (x,y) dentro da grid no formato `X.Y`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VphGkgzUV9WF"},"outputs":[],"source":["@cuda.jit\n","def get_2D_indices(A):\n","    # Pela chamada com '2' em cuda.grid, temos coordenadas unicas X e Y no grid 2D\n","    x, y = cuda.grid(2)\n","    # O codigo acima pode ser entendido como:\n","    # x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n","    # y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n","    \n","    # Escrevendo o indice x somado com o decimo de y\n","    A[x][y] = x + y / 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCG9vecIV9WF"},"outputs":[],"source":["get_2D_indices[blocks, threads_per_block](d_A)\n","result = d_A.copy_to_host()\n","result"]},{"cell_type":"markdown","metadata":{"id":"n3q75gX6V9WF"},"source":["## Exercício: Adição de Matriz Bidimensional Agrupado\n","\n","Nesta célula definimos `n` e criamos uma grid com threads iguais a `n`. Também criamos um vetor de saída com comprimento `n`. Para as entradas criamos vetores de tamanho `stride * n` por razões que serão esclarecidas abaixo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfa4VwcKV9WF"},"outputs":[],"source":["import numpy as np\n","from numba import cuda\n","\n","n = 2048*2048 # 4M\n","\n","# 2D blocks\n","threads_per_block = (32, 32)\n","# 2D grid\n","blocks = (64, 64)\n","\n","# 2048x2048 input matrices\n","a = np.arange(n).reshape(2048,2048).astype(np.float32)\n","b = a.copy().astype(np.float32)\n","\n","# 2048x2048 0-initialized output matrix\n","out = np.zeros_like(a).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_b = cuda.to_device(b)\n","d_out = cuda.to_device(out)"]},{"cell_type":"markdown","metadata":{"id":"bV5WJHzCV9WG"},"source":["### Adicionar Matriz 2D\n","\n","Seu trabalho é completar os TODOs em `matrix_add` para somar corretamente `a` e `b` em `out`. Como um desafio para sua compreensão dos padrões de acesso aglutinados, `matrix_add` aceitará um booleano `coalesced` indicando se os padrões de acesso devem ser aglutinados ou não. Ambos os modos (coalesced e uncoalesced) devem produzir resultados corretos, no entanto, você deve observar acelerações significativas abaixo ao executar com `coalesced` definido como `True`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkGmPKJLV9WG"},"outputs":[],"source":["@cuda.jit\n","def matrix_add(a, b, out, coalesced):\n","    # TODO: set x and y to index correctly such that each thread\n","    # accesses one element in the data.\n","    # x, y \n","    pass\n","    \n","    if coalesced == True:\n","        # TODO: write the sum of one element in `a` and `b` to `out`\n","        # using a coalesced memory access pattern.\n","    else:\n","        # TODO: write the sum of one element in `a` and `b` to `out`\n","        # using an uncoalesced memory access pattern.\n","        "]},{"cell_type":"markdown","metadata":{"id":"rybt8iUqV9WH"},"source":["### Verifique o desempenho\n","Execute ambas as abaixo para iniciar `matrix_add` com os padrões de acesso combinados e não combinados que você escreveu nele e observe a diferença de desempenho. Células adicionais foram fornecidas para confirmar a exatidão do seu kernel.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"db-PFgfwV9WH"},"source":["**Agrupado**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"krTRPVxuV9WH"},"outputs":[],"source":["%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmSjncFUV9WI"},"outputs":[],"source":["result = d_out.copy_to_host()\n","truth = a+b\n","np.array_equal(result, truth)"]},{"cell_type":"markdown","metadata":{"id":"Vsg1CkdmV9WJ"},"source":["**Desagrupado**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oJnFYhsV9WJ"},"outputs":[],"source":["%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpMBO0tPV9WJ"},"outputs":[],"source":["result = d_out.copy_to_host()\n","truth = a+b\n","np.array_equal(result, truth)"]},{"cell_type":"markdown","metadata":{"id":"9oR3Nr-OV9WJ"},"source":["## Memoria compartilhada\n","\n","Até agora, diferenciamos entre memória do host e memória do dispositivo, como se a memória do dispositivo fosse um único tipo de memória. Mas, na verdade, CUDA tem uma [hierarquia de memória](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy) ainda mais refinada . A memória do dispositivo que utilizamos até agora é chamada de **memória global**, que está disponível para qualquer thread ou bloco no dispositivo, pode persistir durante a vida útil do aplicativo e é um espaço de memória relativamente grande.\n","\n","Discutiremos agora como utilizar uma região da memória do dispositivo no chip chamada **memória compartilhada**. A memória compartilhada é um cache definido pelo programador de tamanho limitado que [depende da GPU](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) sendo usado e é **compartilhado** entre todos os threads em um bloco. É um recurso escasso, não pode ser acessado por threads fora do bloco onde foi alocado e não persiste após o término da execução de um kernel. A memória compartilhada, no entanto, tem uma largura de banda muito maior do que a memória global e pode ser usada com grande efeito em muitos kernels, especialmente para otimizar o desempenho.\n","\n","Aqui estão alguns casos de uso comuns para memória compartilhada:\n","\n"," * Memória de cache lida da memória global que precisará ser lida várias vezes em um bloco.\n"," * Buffering da saída dos threads para que possam ser agrupados antes de serem gravados de volta na memória global.\n"," * Dados de preparação para operações de dispersão/reunião dentro de um bloco.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OYKg64oZV9WK"},"source":["### Sintaxe de Memória Compartilhada\n","Numba fornece [funções](https://numba.pydata.org/numba-doc/dev/cuda/memory.html#shared-memory-and-thread-synchronization) para alocar memória compartilhada, bem como para sincronizar entre threads em um bloco, que geralmente é necessário após threads paralelas lerem ou escreverem na memória compartilhada.\n","\n","Ao declarar a memória compartilhada, você fornece a forma da matriz compartilhada, bem como seu tipo, usando um [tipo Numba](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types). **A forma da matriz deve ser um valor constante** e, portanto, você não pode usar argumentos passados para a função ou variáveis fornecidas como `numba.cuda.blockDim.x` ou os valores calculados de `numba.cuda.gridDim`. Aqui está um exemplo para demonstrar a sintaxe com comentários apontando o movimento da memória do host para a memória global do dispositivo, para a memória compartilhada, de volta para a memória global do dispositivo e, finalmente, de volta para a memória do host:"]},{"cell_type":"markdown","metadata":{"id":"_aEQslCSV9WK"},"source":["**Trocar Elementos Usando Memória Compartilhada**\n","O kernel a seguir recebe um vetor de entrada, onde cada thread primeiro escreverá um elemento do vetor na memória compartilhada e, depois de sincronizar de forma que todos os elementos tenham sido gravados na memória compartilhada, gravará um elemento da memória compartilhada na memória trocada vetor de saída.\n","\n","Digno de nota é que cada encadeamento estará escrevendo um valor trocado da memória compartilhada que foi gravado na memória compartilhada por outro encadeamento."]},{"cell_type":"markdown","metadata":{"id":"o4PU2x9NV9WL"},"source":["Usaremos `numba.types` para definir os tipos de valores na memória compartilhada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8zUnoBoV9WL"},"outputs":[],"source":["import numpy as np\n","from numba import types, cuda\n","\n","@cuda.jit\n","def swap_with_shared(vector, swapped):\n","    # Allocate a 4 element vector containing int32 values in shared memory.\n","    temp = cuda.shared.array(4, dtype=types.int32)\n","    \n","    idx = cuda.grid(1)\n","    \n","    # Move an element from global memory into shared memory\n","    temp[idx] = vector[idx]\n","    \n","    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n","    cuda.syncthreads()\n","    #...the following operation is reading an element written to shared memory by another thread.\n","    \n","    # Move an element from shared memory back into global memory\n","    swapped[idx] = temp[3 - cuda.threadIdx.x] # swap elements\n"]},{"cell_type":"markdown","metadata":{"id":"xLdbaZjpV9WL"},"source":["**Criação dos Dados**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMzICg5dV9WM"},"outputs":[],"source":["vector = np.arange(4).astype(np.int32)\n","swapped = np.zeros_like(vector)\n","\n","# Move host memory to device (global) memory\n","d_vector = cuda.to_device(vector)\n","d_swapped = cuda.to_device(swapped)\n","vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qn0qEwjTV9WM"},"outputs":[],"source":["swap_with_shared[1, 4](d_vector, d_swapped)"]},{"cell_type":"markdown","metadata":{"id":"Kz-CHCzIV9WT"},"source":["**Checagem de Resultados**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQwJCvFGV9WU"},"outputs":[],"source":["  # Move device (global) memory back to the host\n","result = d_swapped.copy_to_host()\n","result"]},{"cell_type":"markdown","source":["## Apresentação: Memória Compartilhada para Memória Aglutinada "],"metadata":{"id":"v2OiW7SDaIjO"}},{"cell_type":"code","source":["from IPython.display import IFrame\n","IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/shared_coalescing.pptx', 800, 450)"],"metadata":{"id":"64sOL52VaEkT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yy6rMOA-V9WU"},"source":["## Exercício: memória compartilhada usada para leituras e gravações combinadas com transposição de matriz\n","Neste exercício, você implementará o que acabou de ser demonstrado na apresentação, escrevendo um kernel de transposição de matriz que, usando memória compartilhada, faz leituras e gravações aglutinadas na matriz de saída na memória global."]},{"cell_type":"markdown","metadata":{"id":"j6eilLz2V9WU"},"source":["### Leituras combinadas, gravações não combinadas\n","Como referência e para comparação de desempenho, aqui está um kernel de transposição de matriz ingênua que faz leituras agrupadas da entrada, mas gravações não agrupadas na saída.\n","\n","**Criação de Dados**\n","Aqui criamos uma matriz de entrada 4096x4096 `a`, bem como uma matriz de saída 4096x4096 `transposed` e copiamos para o dispositivo.\n","\n","Também definimos um grid bidimensional com blocos bidimensionais a serem usados abaixo. Observe que criamos um grid com um número total de threads igual ao número de elementos na matriz de entrada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-I8HvmhUV9WV"},"outputs":[],"source":["from numba import cuda\n","import numpy as np\n","\n","n = 4096*4096 # 16M\n","\n","# 2D blocks\n","threads_per_block = (32, 32)\n","#2D grid\n","blocks = (128, 128)\n","\n","# 4096x4096 input and output matrices\n","a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n","transposed = np.zeros_like(a).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_transposed = cuda.to_device(transposed)"]},{"cell_type":"markdown","metadata":{"id":"TvwATwQzV9WW"},"source":["**Núcleo de transposição de matriz ingênua**\n","Este kernel transpõe `a` corretamente, escrevendo a transposição para `transposed`. Ele faz leituras de `a` de maneira combinada, no entanto, suas gravações em `transposed` não são combinadas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nA12uN-V9WW"},"outputs":[],"source":["@cuda.jit\n","def transpose(a, transposed):\n","    x, y = cuda.grid(2)\n","\n","    transposed[x][y] = a[y][x]"]},{"cell_type":"markdown","metadata":{"id":"OOEVg39TV9WW"},"source":["**Chegagem de Performance e Corretude**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPxClxTXV9WX"},"outputs":[],"source":["%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bqt23v5SV9WX"},"outputs":[],"source":["result = d_transposed.copy_to_host()\n","expected = a.T\n","np.array_equal(result, expected)"]},{"cell_type":"markdown","metadata":{"id":"ZXJ438CVV9WX"},"source":["### Refatoração para leituras e gravações combinadas\n","Seu trabalho será refatorar o kernel `transpose` para usar a memória compartilhada e fazer leituras e gravações da memória global de maneira combinada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50uWbBXSV9WX"},"outputs":[],"source":["import numpy as np\n","from numba import cuda, types as numba_types\n","\n","\n","n = 4096*4096 # 16M\n","\n","# 2D blocks\n","threads_per_block = (32, 32)\n","#2D grid\n","blocks = (128, 128)\n","\n","# 4096x4096 input and output matrices\n","a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n","transposed = np.zeros_like(a).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_transposed = cuda.to_device(transposed)"]},{"cell_type":"markdown","metadata":{"id":"yINssfHqV9WY"},"source":["**Exercício: Escreva um Kernel de Transposição que Use Memória Compartilhada**\n","\n","Complete os TODOs dentro da definição do kernel `tile_transpose`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKqWeba1V9WZ"},"outputs":[],"source":["@cuda.jit\n","def tile_transpose(a, transposed):\n","    # `tile_transpose` é o kernel enviado, assumimos que é lançado como um bloco de dimensao 32 x 32,\n","    #e 'a' é um multiplo dessas dimensões.\n","    \n","    # 1) Crie um bloco 32x32 de memoria compartilhada.\n","    # TODO: Seu Código Aqui.\n","    \n","\n","\n","    # Compute o \"offset entre a global e a memo. compartilhada. \n","    # Use o acesso aglutinado/compartilhado que queremos mapear por meio do incremento de threadIdx.x.\n","    # dica: a melhor mudança é mudando o indice dos dados, que nada é a ordem (x, ou y) da nossa matriz \n","    # Nota: `a_col` and `a_row` ja estao corretos!\n","    \n","    a_col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n","    a_row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n","\n","\n","    # 2) Faça a leitura de agrupamento, isto é, a escrita da global para a memória temporaria\n","    # dica: use os indices de thread !\n","    # TODO: Seu Código Aqui.\n","    \n","    \n","    # 3) Espere que todas as threads terminem, para seguir para o próx. passo\n","    # TODO: Seu Código Aqui.\n","\n","\n","\n","    # 4) Calcule o local da transposição para a memoria compartilhada (tile)\n","    # para ser alocada na posicao correta da memoria global. Note que blockIdx.y*blockDim.y\n","    # e blockIdx.x * blockDim.x estão trocadas (porque queremos escrever na posição transposta)\n","    # mas queremos manter o acesso aglutinado, e associar threadIdx.x para a mudança de indice\n","    # mais rapida possivel, de maneira similar com as colunas.\n","    # Nota: `t_col` e `t_row` ja estao corretos !\n","\n","    t_col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n","    t_row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n","\n","    # 5) Escreva da memória compartilhada (usando os  indices da thread)\n","    # de volta para a memória global (usando indices do grid)\n","    # transpondo cada elemento dentro da memória compartilhada.\n","    # TODO: Seu Código Aqui.\n","    \n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"cXg5CCKwV9WZ"},"source":["**Checagem de Performance e Corretude**\n","Verifique o desempenho do seu kernel de transposição refatorado. Você deve ver uma aceleração em comparação com o desempenho de transposição da linha de base acima."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwcu2vQFV9Wa"},"outputs":[],"source":["%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfb7s6OeV9Wa"},"outputs":[],"source":["result = d_transposed.copy_to_host()\n","expected = a.T\n","np.array_equal(result, expected)"]},{"cell_type":"markdown","metadata":{"id":"VvLtI6dTV9Wa"},"source":["### Por que uma melhoria tão pequena?\n","\n","Embora isso seja uma aceleração significativa para apenas algumas linhas de código, você pode pensar que a melhoria de desempenho não é tão forte quanto o esperado com base em melhorias de desempenho anteriores para usar padrões de acesso combinados. Existem 2 razões principais para isso:\n","\n","1. O kernel transpose ingênuo estava fazendo leituras combinadas, portanto, sua versão refatorada otimizou apenas metade do acesso à memória global durante a execução do kernel.\n","2. Seu código como escrito sofre de algo chamado conflitos de banco de memória compartilhada, um tópico para o qual voltaremos nossa atenção agora.\n","\n","## Apresentação: Memória Compartilhada para Memória Aglutinada"]},{"cell_type":"code","source":["from IPython.display import IFrame\n","IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/bank_conflicts.pptx', 800, 450)"],"metadata":{"id":"B890jigCXcku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRjaTyO-V9Wa"},"source":["## Avaliação: resolver conflitos de banco de memória\n","\n","Como exercício final, você refatorará o kernel de transposição utilizando memória compartilhada para ser livre de conflitos de banco de memória compartilhada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVt55PG6V9Wb"},"outputs":[],"source":["import numpy as np\n","from numba import cuda, types\n","\n","n = 4096*4096 # 16M\n","threads_per_block = (32, 32)\n","blocks = (128, 128)\n","\n","a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n","transposed = np.zeros_like(a).astype(np.float32)\n","\n","d_a = cuda.to_device(a)\n","d_transposed = cuda.to_device(transposed)"]},{"cell_type":"markdown","metadata":{"id":"qzaBMSrGV9Wb"},"source":["### Torne o Banco do Kernel Livre de Conflitos\n","\n","O kernel `tile_transpose_conflict_free` é um kernel de transposição de matriz funcional que utiliza memória compartilhada para que leituras e gravações na memória global sejam combinadas. Seu trabalho é refatorar o kernel para que ele não sofra conflitos de banco de memória.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBzvPftkV9Wb"},"outputs":[],"source":["@cuda.jit\n","def tile_transpose_conflict_free(a, transposed):\n","    \n","    # `tile_transpose` é o kernel enviado, assumimos que é lançado como um bloco de dimensao 32 x 32,\n","    #e 'a' é um multiplo dessas dimensões.\n","    \n","    # 1) Crie um bloco 32x32 de memoria compartilhada.\n","    tile = cuda.shared.array((32, 32), numba_types.float32)\n","\n","    # Compute o \"offset entre a global e a memo. compartilhada. \n","    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n","    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n","    \n","    # 2) Faça a leitura de agrupamento, isto é, a escrita da global para a memória temporaria\n","    # Note the use of local thread indices for the shared memory write,\n","    # and global offsets for global memory read.\n","    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a[y, x]\n","\n","    # 3) Espere que todas as threads terminem, para seguir para o próx. passo\n","    cuda.syncthreads()\n","    \n","    # 4) Calcule o local da transposição para a memoria compartilhada (tile)\n","    # para ser alocada na posicao correta da memoria global. \n","    t_x = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n","    t_y = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n","\n","    # 5) Escreva da memória compartilhada (usando os  indices da thread)\n","    # de volta para a memória global (usando indices do grid)\n","    transposed[t_y, t_x] = tile[cuda.threadIdx.x, cuda.threadIdx.y]\n"]},{"cell_type":"markdown","metadata":{"id":"31K1DIudV9Wc"},"source":["### Chegagem de Performance e Corretude\n","Supondo que você tenha resolvido corretamente os conflitos de banco, esse kernel deve executar significativamente mais rápido que o kernel de transposição ingênuo e o kernel de transposição de memória compartilhada (com conflitos de banco). Para passar na avaliação, seu kernel precisará rodar em média em menos de 840 µs.\n","\n","O primeiro valor impresso ao executar a célula a seguir fornecerá o tempo médio de execução do seu kernel.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEFVzO2GV9Wc"},"outputs":[],"source":["%timeit tile_transpose_conflict_free[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYZcuKi5V9Wc"},"outputs":[],"source":["result = d_transposed.copy_to_host()\n","expected = a.T\n","np.array_equal(result, expected)"]},{"cell_type":"markdown","source":[">Seu código deve executar em menos de 840µs!"],"metadata":{"id":"lrlgVfPeayBU"}}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}