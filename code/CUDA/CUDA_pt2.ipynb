{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels CUDA personalizados em Python com Numba\n",
    "\n",
    "Nesta seção, vamos aprofundar nosso entendimento de como o modelo de programação CUDA organiza o trabalho paralelo e aproveitar esse entendimento para escrever **kernels** CUDA, funções que são executadas em paralelo em GPUs CUDA. Os kernels CUDA personalizados, ao utilizar o modelo de programação CUDA, exigem mais trabalho para implementar do que, por exemplo, simplesmente decorar um ufunc com `@vectorize`. No entanto, eles possibilitam computação paralela em lugares onde ufuncs simplesmente não são capazes, e fornecem uma flexibilidade que pode levar ao mais alto nível de desempenho.\n",
    "\n",
    "Esta seção contém três apêndices para aqueles que estão interessados no estudo futher: uma variedade de técnicas de depuração para auxiliar a programação de sua GPU, links para referências de programação CUDA e cobertura de geração de números aleatórios suportados Numba na GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "Quando concluir esta seção, poderá:\n",
    "\n",
    "* Escreva kernels CUDA personalizados em Python e inicie-os com uma configuração de execução.\n",
    "* Utilize loops de passada de grade para trabalhar em paralelo em grandes conjuntos de dados e aproveitar a coalescência de memória.\n",
    "* Use operações atômicas para evitar condições de corrida ao trabalhar em paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A necessidade de kernels personalizados\n",
    "\n",
    "Ufuncs são fantasticamente elegantes, e para qualquer operação escalar que deve ser realizada elemento sábio em dados, ufuncs são provavelmente a ferramenta certa para o trabalho.\n",
    "\n",
    "Como você sabe, há muitas, se não mais, classes de problemas que não podem ser resolvidos aplicando a mesma função a cada elemento de um conjunto de dados. Considere, por exemplo, qualquer problema que exija acesso a mais de um elemento de uma estrutura de dados para calcular sua saída, como algoritmos de *stencil*, ou qualquer problema que não possa ser expresso por um valor de entrada para um mapeamento de valor de saída, como uma redução. Muitos desses problemas ainda são inerentemente paralelizáveis, mas não podem ser expressos por um ufunc.\n",
    "\n",
    "Escrever kernels CUDA personalizados, embora mais desafiadores do que escrever ufuncs acelerados por GPU, fornece aos desenvolvedores uma tremenda flexibilidade para os tipos de funções que podem enviar para serem executados em paralelo na GPU. Além disso, como você vai começar a aprender nesta e na próxima seção, ele também fornece controle refinado sobre *como* o paralelismo é conduzido expondo a hierarquia de threads do CUDA para desenvolvedores explicitamente.\n",
    "\n",
    "Embora permanecendo puramente em Python, a maneira como escrevemos kernels CUDA usando Numba é muito similar de como os desenvolvedores desenvolvem em CUDA C/ C ++. Para aqueles que estão familiarizados com programação em CUDA C/ C ++, você provavelmente vai pegar kernels personalizados em Python com Numba muito rapidamente, e para programadores de linguagem paralela de primeira viagem, sei que o trabalho aqui irá ajudar bem se precisar ou desejar se aprofundar em CUDA em C/C++, ou \n",
    "\n",
    "e para aqueles de vocês aprendê-los pela primeira vez, Sei que o trabalho que você faz aqui também irá atendê-lo bem se você precisar ou desejar desenvolver CUDA em C/C++, ou mesmo, fazer um estudo da riqueza de recursos CUDA na web que são mais comumente retratando CUDA C/C++ código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução aos Kernels CUDA\n",
    "\n",
    "Ao programar no CUDA, os desenvolvedores escrevem funções para a GPU chamadas **kernels**, que são executadas, ou em linguagem CUDA, **launched**, nos muitos núcleos da GPU em  **threads** em paralelo. Quando os kernels são iniciados, os programadores usam uma sintaxe especial, chamada de configuração **execution** (também chamada de configuração de inicialização) para descrever a configuração da execução paralela.\n",
    "\n",
    "Os slides a seguir (que aparecerão após a execução da célula abaixo) fornecem uma introdução de alto nível sobre como os kernels CUDA podem ser criados para trabalhar em grandes conjuntos de dados em paralelo no dispositivo GPU. Trabalhe nos slides e, em seguida, você começará a escrever e executar seus próprios kernels CUDA personalizados, usando as ideias apresentadas nos slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #TODO: adicionar os slides da NVIDIA? -->\n",
    "## O primeiro kernel CUDA\n",
    "\n",
    "Vamos começar com um exemplo concreto e muito simples, reescrevendo nossa função de adição para matrizes NumPy 1D. Os kernels CUDA são compilados usando o decorador `numba.cuda.jit`. `numba.cuda.jit` não deve ser confundido com o decorador `numba.jit` que você já aprendeu que otimiza funções para a CPU.\n",
    "\n",
    "Vamos começar com um exemplo muito simples para destacar algumas das sintaxes essenciais. Vale mencionar que essa função em particular poderia de fato ser escrita como um ufunc, mas nós a escolhemos aqui para manter o foco no aprendizado da sintaxe. Vamos prosseguir para funções mais adequadas para serem escritas como um kernel personalizado abaixo. Certifique-se de ler os comentários com cuidado, pois eles fornecem algumas informações importantes sobre o código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Note que o uso de um array \"out\". Kernels CUDA escritos com @cuda.jit não retornam valores, \n",
    "# assim como suas contrapartes C. Além disso, não há nenhuma atribuição explicita com @cuda.jit.\n",
    "#Diferente do que ocorria em numba.jit.\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    \n",
    "    # Os valores reais das seguintes variáveis fornecidas pelo CUDA para índices de thread e bloco,\n",
    "    # como parâmetros de função, não são conhecidos até que o kernel é lançado.\n",
    "    \n",
    "    # Este cálculo fornece um índice de encadeamento exclusivo dentro de toda a grade (veja os slides acima para mais)\n",
    "    idx = cuda.grid(1)          # 1 = thread grid de 1 dimensao, retorna um valor unico.\n",
    "                                # Esta função conveniencia fornecida pelo Numba é quivalente a\n",
    "                                # `cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x`\n",
    "\n",
    "    # Esta thread fará o trabalho no elemento de dados com o mesmo índice que o seu próprio\n",
    "    # Índice exclusivo na grade.\n",
    "    out[idx] = x[idx] + y[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 4096\n",
    "x = np.arange(n).astype(np.int32) # [0...4095] no host\n",
    "y = np.ones_like(x)               # [1...1] no host\n",
    "\n",
    "d_x = cuda.to_device(x) # Copia do x no device\n",
    "d_y = cuda.to_device(y) # Copia do y no device\n",
    "d_out = cuda.device_array_like(d_x) # Similar ao np.array_like, Mas para arrays no device \n",
    "\n",
    "# Por causa de como escrevemos o kernel acima, precisamos ter um mapeamento de 1 thread \n",
    "# para um elemento de dados,\n",
    "\n",
    "# portanto, definimos o número de threads na grade (128*32) para igual a n (4096).\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n",
    "cuda.synchronize()\n",
    "print(d_out.copy_to_host()) # Should be [1...4096]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faça você mesmo: Ajustar o código\n",
    "\n",
    "Faça as seguintes pequenas alterações no código acima para ver como isso afeta sua execução. Faça suposições sobre o que vai acontecer antes de executar o código:\n",
    "\n",
    "* Diminuir a variável `threads_per_block` \n",
    "* Diminuir a variável `blocks_per_grid` \n",
    "* Aumentar as variáveis `threads_per_block` e/ou `blocks_per_grid\n",
    "* Remover ou comentar a chamada `Cuda.synchronize()` \n",
    "\n",
    "### Resultados\n",
    "\n",
    "No exemplo acima, como o kernel é escrito para que cada thread funcione em exatamente um elemento de dados, é essencial que o número de threads na grade seja igual ao número de elementos de dados.\n",
    "\n",
    "Reduzindo **o número de threads na grade**, reduzindo o número de blocos e/ou reduzindo o número de threads por bloco, há elementos onde o trabalho é deixado de lado e, portanto, podemos ver na saída que os elementos no final do array `d_out` não tinham valores adicionados a ele. Se você editou a configuração de execução reduzindo o número de threads por bloco, então na verdade existem outros elementos através da matriz `d_out` que não foram processados.\n",
    "\n",
    "**Aumentar o tamanho da grade** na verdade cria problemas com acesso à memória fora dos limites. Este erro não será exibido no seu código atualmente, mas mais adiante nesta seção você aprenderá como expor esse erro usando `Cuda-memcheck` e depurá-lo.\n",
    "\n",
    "Você poderia esperar que **remover o ponto de sincronização** resultaria em uma impressão mostrando que nenhum ou menos trabalho foi feito. Este é um palpite razoável, pois sem um ponto de sincronização a CPU funcionará de forma assíncrona enquanto a GPU estiver processando. O detalhe a aprender aqui é que as cópias de memória carregam sincronização implícita, tornando a chamada para `Cuda.synchronize` acima desnecessária."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício: Acelere uma função de CPU como um kernel CUDA personalizado\n",
    "\n",
    "Abaixo está a função escalar da CPU `square_device` que poderia ser usada como uma CPU ufunc. Seu trabalho é refatorá-lo para ser executado como um kernel CUDA com o `@Cuda.Jit`.\n",
    "\n",
    "Você pode pensar que fazer esta função funcionar no dispositivo poderia ser muito mais facilmente feito com `@vectorize`, e você estaria correto. Mas esse cenário lhe dará a chance de trabalhar com toda a sintaxe que introduzimos antes de seguir para exemplos mais complicados e realistas.\n",
    "\n",
    "Neste exercício, terá de:\n",
    "* Refatorar a definição `square_device` para ser um kernel CUDA que fará o trabalho de um thread em um único elemento.\n",
    "* Refazer os arrays `d` e `d_out` abaixo para serem arrays de dispositivos CUDA.\n",
    "* Modificar as variáveis `blocks` e `threads` para valores apropriados para o fornecido `n`.\n",
    "* Refatorar a chamada para `square_device` para ser um lançamento do kernel que inclui uma configuração de execução.\n",
    "\n",
    "O teste de asserção abaixo falhará até que você implemente com sucesso o acima. Se você ficar preso, sinta-se livre para conferir a solução, logo abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #TODO: Coloco a resolução já aqui ? -->\n",
    "### Exercício:  Faça você mesmo !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def square_device(a):\n",
    "    return a**2\n",
    "\n",
    "\n",
    "# Deixe \"n\" fixo para este exercicio\n",
    "n = 4096\n",
    "\n",
    "a = np.arange(n)\n",
    "out = a**2 # Apenas para criterio de comparação\n",
    "\n",
    "\n",
    "d_a = a                  # faça uma função de envio p/ device I \n",
    "d_out = np.zeros_like(a) # faça uma função de envio p/ device II\n",
    "\n",
    "\n",
    "# TODO: Atualize os valores de blocos e threads de acordo com o necessario\n",
    "blocks = 0\n",
    "threads = 0\n",
    "\n",
    "# TODO: Launch as a kernel with an appropriate execution configuration\n",
    "d_out = square_device(d_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import testing\n",
    "testing.assert_almost_equal(d_out, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução - tente antes de visualizar !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor to be a CUDA kernel doing one thread's work.\n",
    "# Don't forget that when using `@cuda.jit`, you must provide an output array as no value will be returned.\n",
    "def square_device(a):\n",
    "    return a**2\n",
    "\n",
    "# Leave the values in this cell fixed for this exercise\n",
    "n = 4096\n",
    "\n",
    "a = np.arange(n)\n",
    "out = a**2\n",
    "\n",
    "d_a = cuda.to_devide(a)             \n",
    "d_out = cuda.device_array_like(d_a) \n",
    "\n",
    "# TODO: Update the execution configuration for the amount of work needed\n",
    "blocks = 128\n",
    "threads = 32\n",
    "\n",
    "d_out = square_device(d_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Um aparte em esconder opções de configuração de latência e execução\n",
    "\n",
    "As GPUs NVIDIA habilitadas para CUDA consistem em vários multiprocessadores de streaming ou *SM's* em um dado, com DRAM conectado. *SM's* contêm todos os recursos necessários para a execução do código do kernel, incluindo muitos núcleos CUDA. Quando um kernel é lançado, cada bloco é atribuído a um único *SM*, com potencialmente muitos blocos atribuídos a um único *SM*. Blocos de partição *SM's* em subdivisões adicionais de 32 threads chamados warps e são esses warps que recebem instruções paralelas para executar.\n",
    "\n",
    "Quando uma instrução leva mais de um ciclo de clock para ser concluída (ou na linguagem CUDA, para **expirar**), o *SM* pode continuar a fazer um trabalho significativo se ainda há warps que estão prontas para reber novas instruções. Por causa de arquivos de registro muito grandes no *SM's*, não há penalidade de tempo para um *SM* para alterar o contexto entre a emissão de instruções para uma warp ou outra. Em suma, a latência das operações pode ser ocultada por *SM's* com outro trabalho significativo, desde que haja outro trabalho a ser feito, de modo que a mesma SM seja reutilizada ao longo da mesma submissão do kernel.\n",
    "\n",
    "Portanto, de importância primária para utilizar todo o potencial da GPU e, assim, escrever aplicativos acelerados de desempenho, é essencial dar ao *SM's* a capacidade de ocultar a latência, fornecendo-lhes um número suficiente de warps que podem ser realizadas mais simplesmente executando núcleos com dimensões de grade e bloco suficientemente grandes.\n",
    "\n",
    "\n",
    "\n",
    "Decidir o melhor tamanho para a grade de thread CUDA é um problema complexo e depende tanto do algoritmo quanto da GPU específica [capacidade de computação] (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-compute-capacidades), mas aqui estão alguns padrões que tendemos a seguir e que podem funcionar bem para começar:\n",
    "\n",
    "  * O tamanho de um bloco deve ser um múltiplo de 32 threads (o tamanho de uma urdidura), com tamanhos de bloco típicos entre 128 e 512 threads por bloco.\n",
    "  * O tamanho da grade deve garantir que a GPU completa seja utilizada sempre que possível. Lançar uma grade onde o número de blocos é 2x-4x o número de *SM's* na GPU é um bom ponto de partida. Algo na faixa de 20 - 100 blocos é geralmente um bom ponto de partida.\n",
    "  * A sobrecarga de lançamento do kernel CUDA aumenta com o número de blocos, portanto, quando o tamanho da entrada é muito grande, achamos melhor não lançar uma grade em que o número de threads seja igual ao número de elementos de entrada, o que resultaria em um grande número de blocos. Em vez disso, usamos um padrão para o qual agora vamos voltar nossa atenção para lidar com grandes entradas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando nos maiores conjuntos de dados com Grid Stride Loops\n",
    "\n",
    "Os slides a seguir fornecem uma visão geral de alto nível de uma técnica chamada **grid stride loop** que criará kernels flexíveis onde cada thread é capaz de trabalhar em mais de um elemento de dados, uma técnica essencial para grandes conjuntos de dados. Execute a célula para carregar os slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #TODO: devo colocar os slides ??  -->\n",
    "from IPython.display import IFrame\n",
    "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/AC_CUDA_Python_2.pptx', 640, 390)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Um primeiro loop de Stride Grid\n",
    "\n",
    "Vamos refatorar o `add_kernel` acima para utilizar um loop de stride Grid para que possamos lançá-lo para trabalhar em conjuntos de dados maiores de forma flexível, incorrendo nos benefícios da coalescência global de **memória**, que permite que threads paralelos acessem memória em partes contíguas, Um cenário que a GPU pode utilizar para reduzir o número total de operações de memória:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    \n",
    "\n",
    "    start = cuda.grid(1) # `cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x`\n",
    "    \n",
    "    # This calculation gives the total number of threads in the entire grid\n",
    "    stride = cuda.gridsize(1)   # 1 =  thread grid de uma dimensao, returns a single value.\n",
    "                                # Conveniencia do Numba, que pode ser obtida por:\n",
    "                                # `cuda.blockDim.x * cuda.gridDim.x`\n",
    "\n",
    "  # Este thread começará a trabalhar no índice do elemento de dados igual ao seu próprio\n",
    "  # índice exclusivo na grade e, em seguida, passará o número de threads na grade cada iteração, \n",
    "  # desde que não tenha saído dos limites dos dados. Dessa forma, cada encadeamento\n",
    "  # pode funcionar em mais de um elemento de dados e, juntos, todos os encadeamentos \n",
    "  # funcionarão em todos os elementos de dados.\n",
    "\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        # Assuming x and y inputs are same length\n",
    "        out[i] = x[i] + y[i]\n",
    "\n",
    "\n",
    "\n",
    "n = 100000 # Esse valor é muito mais threads que temos no grid\n",
    "x = np.arange(n).astype(np.int32)\n",
    "y = np.ones_like(x)\n",
    "\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_out = cuda.device_array_like(d_x)\n",
    "\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n",
    "print(d_out.copy_to_host()) # Remember, memory copy carries imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício: Implementar um Loop de Stride Grid \n",
    "\n",
    "Refatore a seguinte função escalar `hypot_stride` da CPU para executar como um kernel CUDA utilizando um loop de Stride Grid. Sinta-se livre para olhar a solução, logo abaixo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import hypot\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "def hypot_stride(a, b, c):\n",
    "    c = hypot(a, b)\n",
    "\n",
    "\n",
    "# Voce não precisa modificar nada neste bloco de comandos !\n",
    "n = 1000000\n",
    "a = np.random.uniform(-12, 12, n).astype(np.float32)\n",
    "b = np.random.uniform(-12, 12, n).astype(np.float32)\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.device_array_like(d_b)\n",
    "\n",
    "blocks = 128\n",
    "threads_per_block = 64\n",
    "\n",
    "hypot_stride[blocks, threads_per_block](d_a, d_b, d_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de implementação, caso dê falha tente modificar algo faltante\n",
    "from numpy import testing\n",
    "testing.assert_almost_equal(np.hypot(a,b), d_c.copy_to_host(), decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import hypot\n",
    "\n",
    "@cuda.jit\n",
    "def hypot_stride(a, b, c):\n",
    "        # Primeira maneira de Resolucao\n",
    "    start = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "        # Segunda maneira de Resolucao\n",
    "    # start  = cuda.threadIdx + cuda.blockDim.x * cuda.blockIdx.x\n",
    "    # stride = cuda.blockDim.x * cuda.GridDim.x\n",
    "    for i in range(start, a.shape[0], stride): \n",
    "        c[i] = hypot(a[i], b[i])\n",
    "\n",
    "# Voce não precisa modificar nada neste bloco de comandos !\n",
    "n = 1000000\n",
    "a = np.random.uniform(-12, 12, n).astype(np.float32)\n",
    "b = np.random.uniform(-12, 12, n).astype(np.float32)\n",
    "d_a = cuda.to_device(a)\n",
    "d_b = cuda.to_device(b)\n",
    "d_c = cuda.device_array_like(d_b)\n",
    "\n",
    "blocks = 128\n",
    "threads_per_block = 64\n",
    "\n",
    "hypot_stride[blocks, threads_per_block](d_a, d_b, d_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cronometrando o Kernel\n",
    "\n",
    "Vamos aproveitar o tempo para fazer algum tempo de desempenho para o kernel `hypot_stride` . Se você não conseguiu implementá-lo com sucesso, copie e execute a solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linha de base da CPU\n",
    "\n",
    "Primeiro vamos obter uma linha de base com `np.hypot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.hypot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba na CPU\n",
    "\n",
    "Em seguida, vamos ver sobre uma versão otimizada para CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def numba_hypot(a, b):\n",
    "    return np.hypot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit numba_hypot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispositivo Single Thread\n",
    "\n",
    "Só para ver, vamos lançar nosso kernel em uma grade com apenas um único thread. Aqui usaremos `%time`, que só executa a instrução uma vez para garantir que nossa medição não seja afetada pela profundidade finita da fila do kernel CUDA. Iremos também adicionar um `Cuda.synchronize` para termos a certeza que não obtemos quaisquer tempos inócuos por causa do retorno do controlo à CPU, onde está o temporizador, antes do kernel terminar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%time hypot_stride[1, 1](d_a, d_b, d_c); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espero que não seja uma grande surpresa que isso seja muito mais lento do que até mesmo a execução da CPU de linha de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralelismo no Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time hypot_stride[128, 64](d_a, d_b, d_c); cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações Atômicas e Evitando Condições de Corrida\n",
    "\n",
    "CUDA, como muitos frameworks de execução paralela de propósito geral, torna possível ter condições de corrida em seu código.  Uma condição de corrida no CUDA surge quando threads leem ou escrevem de um local de memória que pode ser modificado por outro thread independente. De um modo geral, você precisa se preocupar com:\n",
    "\n",
    " * riscos de leitura após gravação: um thread está lendo um local de memória ao mesmo tempo que outro thread pode estar escrevendo nele.\n",
    " * write-after-write (WAW) hazards: Dois threads estão gravando no mesmo local de memória, e apenas uma gravação será visível quando o kernel estiver completo.\n",
    " \n",
    "Uma estratégia comum para evitar esses dois riscos é organizar o algoritmo do kernel CUDA de modo que cada encadeamento tenha responsabilidade exclusiva por subconjuntos exclusivos de elementos de matriz de saída e/ou nunca usar a mesma matriz para entrada e saída em uma única chamada do kernel. (Algoritmos iterativos podem usar uma estratégia de buffer duplo, se necessário, e alternar matrizes de entrada e saída em cada iteração.)\n",
    "\n",
    "No entanto, há muitos casos em que diferentes threads precisam combinar resultados. Considere algo muito simples, como: \"cada thread incrementa um contador global.\" A implementação disto no seu kernel requer que cada thread:\n",
    "\n",
    "1. Leia o valor actual de um contador global.\n",
    "2. Calcular `contador + 1`.\n",
    "3. Escreva esse valor de volta à memória global.\n",
    "\n",
    "No entanto, não há garantia de que outro encadeamento não tenha alterado o contador global entre as etapas 1 e 3. Para resolver esse problema, o CUDA fornece operações **atômicas** que lerão, modificarão e atualizarão um local de memória em uma etapa indivisível. Numba suporta várias dessas funções, [descritas aqui](http://numba.pydata.org/numba-doc/dev/cuda/intrinsics.html#supported-atomic-operations).\n",
    "\n",
    "Vamos fazer o nosso kernel do contador de threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def thread_counter_race_condition(global_counter):\n",
    "    global_counter[0] += 1  # Isso é ruim !\n",
    "    \n",
    "@cuda.jit\n",
    "def thread_counter_safe(global_counter):\n",
    "    cuda.atomic.add(global_counter, 0, 1)  # Adiciona seguramente +1 no contador global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devolve o resultado incorreto\n",
    "global_counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
    "thread_counter_race_condition[64, 64](global_counter)\n",
    "\n",
    "print('Should be %d:' % (64*64), global_counter.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciona Corretamente\n",
    "global_counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
    "thread_counter_safe[64, 64](global_counter)\n",
    "\n",
    "print('Should be %d:' % (64*64), global_counter.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafio : Escreva um kernel de histograma acelerado !\n",
    "Este exercício é considerado um desafio, pois será necessário fazer uso do que já aprendeu até aqui !\n",
    "Tente elaborar este exercício levando em conta o exemplo (execução em CPU) na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_histogram(x, xmin, xmax, histogram_out):\n",
    "  '''Incrementar contagens no histogram_out, dado intervalo de histograma [xmin, xmax).'''\n",
    "    # Note que não temos que passar em nbins explicitamente, porque o tamanho do histogram_out determina isso\n",
    "  nbins = histogram_out.shape[0]\n",
    "  bin_width = (xmax - xmin) / nbins\n",
    "    \n",
    "  # Essa é uma maneira bem lenta de fazer isso com NumPy\n",
    "  # mas parece similar com o que fará na GPU\n",
    "  for element in x:\n",
    "      bin_number = np.int32((element - xmin)/bin_width)\n",
    "      if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
    "          # only increment if in range\n",
    "          histogram_out[bin_number] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cuda_histogram(x, xmin, xmax, histogram_out):\n",
    "  #insira sua implementação aqui !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x = cuda.to_device(x)\n",
    "d_histogram_out = cuda.to_device(np.zeros(shape=10, dtype=np.int32))\n",
    "\n",
    "blocks = 128\n",
    "threads_per_block = 64\n",
    "\n",
    "cuda_histogram[blocks, threads_per_block](d_x, xmin, xmax, d_histogram_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cuda_histogram(x, xmin, xmax, histogram_out):\n",
    "  nbins = histogram_out.shape[0]\n",
    "  bin_width = (xmax - xmin) / nbins\n",
    "\n",
    "  start = cuda.grid(1)\n",
    "  stride = cuda.gridsize(1)\n",
    "      # 2ª maneira - decl. das variaveis\n",
    "  # start = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x\n",
    "  # stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "\n",
    "  for element in range(start, nbins, stride):\n",
    "      bin_number = np.int32((element - xmin)/bin_width)\n",
    "      if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
    "        cuda.atomic.add(histogram_out, bin_number, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
