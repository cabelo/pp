{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBl_9yStW0cz"
   },
   "source": [
    "# Introdução ao CUDA\n",
    "\n",
    "Esta pode ser considerada uma introdução mínima ao CUDA e às possibilidades de uso neste ambiente. \n",
    "\n",
    "---\n",
    "##  Configurando e instalando \n",
    "\n",
    "Para acessar GPUs NVIDIA no Google Colab é preciso alterar a configuração do ambiente. Para isso, clique no menu superior em **Runtime > Change runtime type**, em **Hardware accelerator** selecione **GPU** e clique em **Save**. Esta configuração é persistente, ou seja, o arquivo já será inicializado neste ambiente da próxima vez que for aberto. \n",
    "\n",
    "O compilador CUDA `nvcc` já vem instalado e pode ser invocado como vemos abaixo, solicitando sua versão com o parâmetro `--version`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1063,
     "status": "ok",
     "timestamp": 1667307703978,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "rPPSIrkTVO35",
    "outputId": "3af38541-0a94-4236-d6f9-c460fdf35c69"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkG3-jzTc5P3"
   },
   "source": [
    "O comando e `nvidia-smi` _(Systems Management Interface)_ nos dá detalhes do ambiente, tais como aceleradores disponíveis e processos em execução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1667307719778,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "5HgjH3lddEaR",
    "outputId": "1e6f3e0f-e7b5-462f-98fb-6d2f4ccce567"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7LU147xdQlI"
   },
   "source": [
    "Para facilitar ainda mais, vamos instalar a extensão `nvcc_plugin` que permite escrever e executar código CUDA diretamente nas células do Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5127,
     "status": "ok",
     "timestamp": 1667307766872,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "DlKyxRLAdXKs",
    "outputId": "43460365-d576-41a2-ac86-7a654008ad81"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "%load_ext nvcc_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROGtfdUhdZty"
   },
   "source": [
    "Depois disso, basta usar o prefixo `%%cu` no início da célula para rodar código CUDA diretamente no Jupyter sem a necessidade de invocar explicitamente o compilador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2719,
     "status": "ok",
     "timestamp": 1667307777506,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "8dISM5S7VcKI",
    "outputId": "ca5b18b9-9c1e-49a5-ca6e-f701d56074cf"
   },
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <iostream>\n",
    "int main() {\n",
    "  int nDevices;\n",
    "\tstd::cout << \"Welcome to CUDA!\" << std::endl;\n",
    "  cudaGetDeviceCount(&nDevices);\n",
    "  for (int i = 0; i < nDevices; i++) {\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, i);\n",
    "    std::cout << \"Device Number: \" << i << std::endl;\n",
    "    std::cout << \"  Device name: \" << prop.name << std::endl;\n",
    "    std::cout << \"  Memory Clock Rate (KHz): \" << prop.memoryClockRate << std::endl;\n",
    "    std::cout << \"  Memory Bus Width (bits): \" << prop.memoryBusWidth << std::endl;\n",
    "    std::cout << \"  Peak Memory Bandwidth (GB/s): \" << 2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6 << std::endl;\n",
    "  }\n",
    "\treturn 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2l60EtMeAhj"
   },
   "source": [
    "---\n",
    "## Pré-requisitos\n",
    "\n",
    "Para aproveitar ao máximo este laboratório, você já deve ser capaz de:\n",
    "\n",
    "- Declarar variáveis, escrever loops e usar instruções `if/else` em C.\n",
    "- Definir e invocar funções em C.\n",
    "- Alocar arrays em C.\n",
    "\n",
    "Nenhum conhecimento prévio de CUDA é necessário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHO9Azzd5NjS"
   },
   "source": [
    "---\n",
    "## Objetivos\n",
    "\n",
    "Ao concluir este laboratório, você será capaz de:\n",
    "\n",
    "- Escrever, compilar e executar programas C/C++ que chamam funções de CPU e **disparam** **kernels** em GPU.\n",
    "- Controlar a **hierarquia de threads** paralela usando a **configuração de execução**.\n",
    "- Refatorar loops seriais para executar suas iterações em paralelo em uma GPU.\n",
    "- Alocar e liberar memória disponível para CPUs e GPUs.\n",
    "- Manipular erros gerados pelo código CUDA.\n",
    "- Acelerar aplicativos somente de CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEov_tLu6PJg"
   },
   "source": [
    "---\n",
    "## Escrevendo o código da aplicação para a GPU\n",
    "\n",
    "CUDA fornece extensões para muitas linguagens de programação comuns, no caso deste laboratório, C/C++. Essas extensões de linguagem permitem que os desenvolvedores executem facilmente funções em seu código-fonte em uma GPU.\n",
    "\n",
    "Abaixo está um arquivo `.cu` (extensão do arquivo para programas acelerados por CUDA). Ele contém duas funções, a primeira que será executada na CPU, a segunda que será executada na GPU. Gaste um tempo identificando as diferenças entre as funções, tanto em termos de como elas são definidas quanto de como são invocadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1667307795961,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "Ond3PA7O7UYS",
    "outputId": "8e254e99-2e3e-4792-b3f2-6555b026c67e"
   },
   "outputs": [],
   "source": [
    "%%writefile 01-hello-gpu.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "void CPUFunction() {\n",
    "  printf(\"Esta função está definida para ser executada na CPU.\\n\");\n",
    "}\n",
    "\n",
    "__global__ void GPUFunction() {\n",
    "  printf(\"Esta função está definida para ser executada na GPU.\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  CPUFunction();\n",
    "  GPUFunction<<<1, 1>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR3KDWbl7aUk"
   },
   "source": [
    "Aqui estão algumas linhas de código importantes para destacar, bem como alguns outros termos comuns usados ​​em computação acelerada:\n",
    "\n",
    "`__global__ void GPUFunction()`\n",
    "  - A palavra-chave `__global__` indica que a seguinte função será executada na GPU e pode ser invocada **globalmente**, o que neste contexto significa tanto pela CPU quanto pela GPU.\n",
    "  - Muitas vezes, o código executado na CPU é chamado de código do **host**, e o código executado na GPU é chamado de código do **dispositivo** ou **acelerador**.\n",
    "  - Observe o tipo de retorno `void`. É necessário que as funções definidas com a palavra-chave `__global__` retornem o tipo `void`.\n",
    "\n",
    "`GPUFunction<<<1, 1>>>();`\n",
    "  - Normalmente, ao chamar uma função para execução na GPU, chamamos essa função de **kernel**, que é **disparada** ou **lançada**.\n",
    "  - Ao lançar um kernel, devemos fornecer uma **configuração de execução**, que é feita usando a sintaxe `<<< ... >>>` antes de passar ao kernel quaisquer argumentos esperados.\n",
    "  - Em um nível alto, a configuração de execução permite que os programadores especifiquem a **hierarquia de threads** para uma inicialização do kernel, que define o número de agrupamentos de threads (chamados **blocos**), bem como quantos **threads** para executar em cada bloco. A configuração de execução será explorada detalhadamente mais adiante no laboratório, mas, por enquanto, observe que o kernel está sendo iniciado com `1` bloco de threads (o primeiro argumento de configuração de execução) que contém `1` thread (o segundo argumento de configuração) .\n",
    "\n",
    "`cudaDeviceSynchronize();`\n",
    "  - Ao contrário de muitos códigos C/C++, o lançamento de kernels é **assíncrono**: o código da CPU continuará a ser executado *sem esperar que o lançamento do kernel seja concluído*.\n",
    "  - Uma chamada para `cudaDeviceSynchronize`, uma função fornecida pelo runtime CUDA, fará com que o código do host (CPU) espere até que o código do dispositivo (GPU) seja concluído e só então retome a execução na CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI5h7LvV-XVX"
   },
   "source": [
    "---\n",
    "## Experimente você!\n",
    "\n",
    "Abaixo invocamos o compilador `nvcc` para compilar e executar (usando o parâmetro `-run`) o arquivo criado na célula anterior. Execute a célula abaixo para ver o resultado... \n",
    "\n",
    "O parâmetro `-arch=` permite escolher a arquitetura alvo da GPU, experimente trocar para `sm_50` e veja o que acontece. \n",
    "\n",
    "Volte na célula que gerou o arquivo e experimente mudar o número de blocos e threads. Não esqueça de executar a célula para salvar novamente o arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1452,
     "status": "ok",
     "timestamp": 1667307811731,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "IJ5C8pxgWa_5",
    "outputId": "15b6c10f-13b1-4882-e8ff-280c17486076"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o hello-gpu 01-hello-gpu.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvXwN0OMBG-7"
   },
   "source": [
    "---\n",
    "## Índices de threads e blocos\n",
    "\n",
    "Cada thread recebe um índice dentro de seu bloco de threads, começando em `0`. Além disso, cada bloco recebe um índice, começando em '0'. Assim como os encadeamentos são agrupados em blocos de encadeamentos, os blocos são agrupados em uma **grade (grid)**, que é a entidade mais alta na hierarquia de encadeamentos CUDA. Em resumo, os kernels CUDA são executados em uma grade de 1 ou mais blocos, com cada bloco contendo o mesmo número de 1 ou mais threads.\n",
    "\n",
    "Kernels CUDA têm acesso a variáveis ​​especiais que identificam tanto o índice da thread (dentro do bloco) que está executando o kernel, quanto o índice do bloco (dentro da grade) em que a thread está. Essas variáveis ​​são `threadIdx.x` e `blockIdx.x` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1667307842019,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "9UQu-f1xBVrf",
    "outputId": "629f88c5-3909-4123-e4d6-46bf38ab4d4c"
   },
   "outputs": [],
   "source": [
    "%%writefile 02-thread-and-block-idx.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void printSuccessForCorrectExecutionConfiguration() {\n",
    "  if(threadIdx.x == 1023 && blockIdx.x == 255)\n",
    "    printf(\"Success!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  /* This is one possible execution context that will make\n",
    "   * the kernel launch print its success message. */\n",
    "  printSuccessForCorrectExecutionConfiguration<<<256, 1024>>>();\n",
    "\n",
    "  /* Don't forget kernel execution is asynchronous and you must\n",
    "   * sync on its completion. */\n",
    "  cudaDeviceSynchronize();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1877,
     "status": "ok",
     "timestamp": 1667307846101,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "_niddzloB3VK",
    "outputId": "29172228-e427-4ebb-93fa-e6049a433d58"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o thread-and-block-idx 02-thread-and-block-idx.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4T9a5AFDU6j"
   },
   "source": [
    "---\n",
    "## Acelerando loops `for` \n",
    "\n",
    "Loops `for` em aplicativos somente de CPU estão prontos para aceleração: em vez de executar cada iteração do loop em série, cada iteração do loop pode ser executada em paralelo em seu próprio thread. Considere o seguinte loop for e observe, embora seja óbvio, que ele controla quantas vezes o loop será executado, além de definir o que acontecerá para cada iteração do loop:\n",
    "\n",
    "```cpp\n",
    "int N = 2<<20;\n",
    "for (int i = 0; i < N; ++i) {\n",
    "  printf(\"%d\\n\", i);\n",
    "}\n",
    "```\n",
    "\n",
    "Para paralelizar este loop, 2 passos devem ser seguidos:\n",
    "\n",
    "- Um kernel deve ser escrito para fazer o trabalho de uma **única iteração do loop**.\n",
    "- Como o kernel será independente de outros kernels em execução, a configuração de execução deve ser tal que o kernel execute o número correto de vezes, por exemplo, o número de vezes que o loop teria iterado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1667308778082,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "Kd67suEUV2jq",
    "outputId": "326b74be-c8b1-47e4-834d-8300114e976a"
   },
   "outputs": [],
   "source": [
    "%%writefile 03-single-block-loop.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "/*\n",
    " * Notice the absence of the previously expected argument `N`.\n",
    " */\n",
    "\n",
    "__global__ void loop() {\n",
    "  /* This kernel does the work of only 1 iteration\n",
    "   * of the original for loop. Indication of which\n",
    "   * \"iteration\" is being executed by this kernel is\n",
    "   * still available via `threadIdx.x`.  */\n",
    "  printf(\"This is iteration number %d\\n\", threadIdx.x);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  /* It is the execution context that sets how many \"iterations\"\n",
    "   * of the \"loop\" will be done.\n",
    "   */\n",
    "  loop<<<1, 10>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2503,
     "status": "ok",
     "timestamp": 1667308785192,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "ve8rYwyOWIhx"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o single-block-loop 03-single-block-loop.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "760U_zyFWt-j"
   },
   "source": [
    "---\n",
    "## Usando dimensões de bloco para mais paralelização\n",
    "\n",
    "Há um limite para o número de threads que podem existir em um bloco de thread: 1024 para ser preciso. Para aumentar a quantidade de paralelismo em aplicativos acelerados, devemos ser capazes de coordenar entre vários blocos de threads.\n",
    "\n",
    "Kernels CUDA têm acesso a uma variável especial que fornece o número de threads em um bloco: `blockDim.x`. Usando essa variável, em conjunto com `blockIdx.x` e `threadIdx.x`, a paralelização aumentada pode ser realizada organizando a execução paralela em vários blocos de vários threads com a expressão idiomática `threadIdx.x + blockIdx.x * blockDim.x `. Aqui está um exemplo detalhado.\n",
    "\n",
    "A configuração de execução `<<<10, 10>>>` lançaria um grid com um total de 100 threads, contidos em 10 blocos de 10 threads. Esperamos, portanto, que cada thread tenha a capacidade de calcular algum índice exclusivo entre `0` e `99`.\n",
    "\n",
    "- Se o bloco `blockIdx.x` for igual a `0`, então `blockIdx.x * blockDim.x` será `0`. Adicionando a `0` os possíveis valores `threadIdx.x` `0` a `9`, então podemos gerar os índices `0` a `9` dentro da grade de 100 threads.\n",
    "- Se o bloco `blockIdx.x` for igual a `1`, então `blockIdx.x * blockDim.x` será `10`. Adicionando a `10` os possíveis valores `threadIdx.x` `0` a `9`, então podemos gerar os índices `10` a `19` dentro da grade de 100 threads.\n",
    "- Se o bloco `blockIdx.x` for igual a `5`, então `blockIdx.x * blockDim.x` será `50`. Adicionando a `50` os possíveis valores `threadIdx.x` de `0` a `9`, podemos gerar os índices de `50` a `59` dentro da grade de 100 threads.\n",
    "- Se o bloco `blockIdx.x` for igual a `9`, então `blockIdx.x * blockDim.x` será `90`. Adicionando a `90` os possíveis valores `threadIdx.x` `0` a `9`, então podemos gerar os índices `90` a `99` dentro da grade de 100 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1667309141685,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "SuWsdxjQXXu5",
    "outputId": "2dc6f1a1-b9a1-40f4-abd0-d9d3d7ea796d"
   },
   "outputs": [],
   "source": [
    "%%writefile 04-multi-block-loop.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void loop() {\n",
    "  /* This idiomatic expression gives each thread\n",
    "   * a unique index within the entire grid.\n",
    "   */\n",
    "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  printf(\"%d\\n\", i);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  /* Additional execution configurations that would\n",
    "   * work and meet the exercises contraints are:\n",
    "   * <<<5, 2>>>\n",
    "   * <<<10, 1>>> */\n",
    "  loop<<<2, 5>>>();\n",
    "  cudaDeviceSynchronize();\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1383,
     "status": "ok",
     "timestamp": 1667309158968,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "mox8jBd9Xlka"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o multi-block-loop 04-multi-block-loop.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiY6WGLKX97C"
   },
   "source": [
    "---\n",
    "## Alocação de memória a ser acessada na GPU e na CPU\n",
    "\n",
    "Versões mais recentes do CUDA (versão 6 e posterior) facilitaram a alocação de memória disponível tanto para o host da CPU quanto para qualquer número de dispositivos GPU e, embora existam muitas [técnicas intermediárias e avançadas](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations) para gerenciamento de memória que oferecerá suporte ao melhor desempenho em aplicativos acelerados, a técnica de gerenciamento de memória CUDA mais básica que abordaremos agora oferece suporte a ganhos de desempenho fantásticos em aplicativos somente de CPU com quase nenhuma sobrecarga de desenvolvedor.\n",
    "\n",
    "Para alocar e liberar memória e obter um ponteiro que possa ser referenciado no código do host e do dispositivo, substitua as chamadas para `malloc` e `free` por `cudaMallocManaged` e `cudaFree` como no exemplo a seguir:\n",
    "\n",
    "\n",
    "```cpp\n",
    "// CPU-only\n",
    "int N = 2<<20;\n",
    "size_t size = N * sizeof(int);\n",
    "\n",
    "int *a;\n",
    "a = (int *)malloc(size);\n",
    "\n",
    "// Use `a` in CPU-only program.\n",
    "free(a);\n",
    "```\n",
    "\n",
    "```cpp\n",
    "// Accelerated\n",
    "int N = 2<<20;\n",
    "size_t size = N * sizeof(int);\n",
    "\n",
    "int *a;\n",
    "// Note the address of `a` is passed as first argument.\n",
    "cudaMallocManaged(&a, size);\n",
    "\n",
    "// Use `a` on the CPU and/or on any GPU in the accelerated system.\n",
    "cudaFree(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667309592578,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "vLqWyytXYlyH",
    "outputId": "4fa0a076-efc5-422c-b483-76040663c04c"
   },
   "outputs": [],
   "source": [
    "%%writefile 05-double-elements.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "void init(int *a, int N) {\n",
    "  int i;\n",
    "  for (i = 0; i < N; ++i) \n",
    "    a[i] = i;\n",
    "}\n",
    "\n",
    "__global__ void doubleElements(int *a, int N) {\n",
    "  int i;\n",
    "  i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (i < N)\n",
    "    a[i] *= 2;\n",
    "}\n",
    "\n",
    "bool checkElementsAreDoubled(int *a, int N) {\n",
    "  int i;\n",
    "  for (i = 0; i < N; ++i)\n",
    "    if (a[i] != i*2) \n",
    "      return false;\n",
    "  return true;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int N = 1000;\n",
    "  int *a;\n",
    "\n",
    "  size_t size = N * sizeof(int);\n",
    "  /* Use `cudaMallocManaged` to allocate pointer `a` available\n",
    "   * on both the host and the device. */\n",
    "\n",
    "  cudaMallocManaged(&a, size);\n",
    "  init(a, N);\n",
    "\n",
    "  size_t threads_per_block = 256;\n",
    "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
    "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
    "\n",
    "  /* Use `cudaFree` to free memory allocated with `cudaMallocManaged`. */\n",
    "  cudaFree(a);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2682,
     "status": "ok",
     "timestamp": 1667309600259,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "1I9iWhbUZEN1",
    "outputId": "d897d113-50b8-4a76-87c7-fe8071d5a6ae"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o double-elements 05-double-elements.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_hRvjuIZ-tP"
   },
   "source": [
    "---\n",
    "## Tratamento de incompatibilidades de configuração de bloco com o número de threads necessários\n",
    "\n",
    "Pode ser que uma configuração de execução não possa ser expressa para criar o número exato de threads necessários para paralelizar um loop.\n",
    "\n",
    "Um exemplo comum tem a ver com o desejo de escolher tamanhos de bloco ideais. Por exemplo, devido às características de hardware da GPU, os blocos que contêm vários threads que são múltiplos de 32 geralmente são desejáveis ​​para benefícios de desempenho. Supondo que queríamos lançar blocos cada um contendo 256 threads (um múltiplo de 32) e precisávamos executar 1.000 tarefas paralelas (um número trivialmente pequeno para facilitar a explicação), então não há número de blocos que produziria um total exato de 1000 threads na grade, pois não há valor inteiro 32 pode ser multiplicado por exatamente 1000.\n",
    "\n",
    "Este cenário pode ser facilmente resolvido da seguinte maneira:\n",
    "\n",
    "- Escreva uma configuração de execução que crie **mais** threads do que o necessário para realizar o trabalho alocado.\n",
    "- Passar um valor como argumento no kernel (`N`) que representa o tamanho total do conjunto de dados a ser processado, ou o total de threads que são necessários para concluir o trabalho.\n",
    "- Após calcular o índice da thread dentro da grade (usando `tid+bid*bdim`), verifique se este índice não excede `N`, e só execute o trabalho pertinente do kernel se não exceder.\n",
    "\n",
    "Aqui está um exemplo de uma maneira idiomática de escrever uma configuração de execução quando tanto `N` quanto o número de threads em um bloco são conhecidos e uma correspondência exata entre o número de threads na grade e `N` não pode ser garantida. Ele garante que sempre haja pelo menos tantos encadeamentos quantos forem necessários para `N`, e apenas 1 bloco adicional de encadeamentos extras, no máximo:\n",
    "\n",
    "```cpp\n",
    "// Suponha que `N` seja conhecido\n",
    "int N = 100000;\n",
    "\n",
    "// Suponha que desejamos definir `threads_per_block` exatamente como `256`\n",
    "size_t threads_per_block = 256;\n",
    "\n",
    "// Certifique-se de que haja pelo menos `N` threads na grade, com apenas 1 bloco excedente\n",
    "size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "some_kernel<<<number_of_blocks, threads_per_block>>>(N);\n",
    "```\n",
    "\n",
    "Como a configuração de execução acima resulta em mais threads na grade do que `N`, deve-se tomar cuidado dentro da definição de `some_kernel` para que `some_kernel` não tente acessar elementos de dados fora do intervalo, ao ser executado por um dos tópicos \"extras\":\n",
    "\n",
    "```cpp \n",
    "__global__ some_kernel(int N) {\n",
    "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  if (idx < N) { // Verifica se `idx` mapeia para algum valor dentro de `N`\n",
    "    // Só executa em caso verdadeiro\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1667310438966,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "cb5iL-65cCDN",
    "outputId": "d0975eae-30b1-4458-b6e3-6188823fe413"
   },
   "outputs": [],
   "source": [
    "%%writefile 06-mismatched-config-loop.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void initializeElementsTo(int initialValue, int *a, int N) {\n",
    "  int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  if (i < N) \n",
    "    a[i] = initialValue;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  /* Do not modify `N`.  */\n",
    "  int N = 1000;\n",
    "\n",
    "  int *a;\n",
    "  size_t size = N * sizeof(int);\n",
    "\n",
    "  cudaMallocManaged(&a, size);\n",
    "\n",
    "  /* Assume we have reason to want the number of threads\n",
    "   * fixed at `256`: do not modify `threads_per_block`. */\n",
    "  size_t threads_per_block = 256;\n",
    "\n",
    "  /* The following is idiomatic CUDA to make sure there are at\n",
    "   * least as many threads in the grid as there are `N` elements. */\n",
    "  size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "  int initialValue = 6;\n",
    "\n",
    "  initializeElementsTo<<<number_of_blocks, threads_per_block>>>(initialValue, a, N);\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  /* Check to make sure all values in `a`, were initialized. */\n",
    "  for (int i = 0; i < N; ++i) \n",
    "    if(a[i] != initialValue) {\n",
    "      printf(\"FAILURE: target value: %d\\t a[%d]: %d\\n\", initialValue, i, a[i]);\n",
    "      cudaFree(a);\n",
    "      exit(1);\n",
    "    }\n",
    "  printf(\"SUCCESS!\\n\");\n",
    "  cudaFree(a);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWq-PRRyclAg"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o mismatched-config-loop 06-mismatched-config-loop.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cz1hUbv3dJuH"
   },
   "source": [
    "---\n",
    "## Conjuntos de dados maiores que a grade\n",
    "\n",
    "Por opção, geralmente para criar a configuração de execução com melhor desempenho, ou por necessidade, o número de threads em uma grade pode ser menor que o tamanho de um conjunto de dados. Considere uma matriz com 1000 elementos e uma grade com 250 threads (usando tamanhos triviais aqui para facilitar a explicação). Aqui, cada thread na grade precisará ser usado 4 vezes. Um método comum para fazer isso é usar um **grid-stride loop** dentro do kernel.\n",
    "\n",
    "Em um loop grid-stride, cada thread calculará seu índice exclusivo dentro da grade usando `tid+bid*bdim`, realizará sua operação no elemento naquele índice dentro da matriz e, em seguida, adicionará ao seu índice o número de threads na grade e repete isso, até que esteja fora do alcance da matriz. Por exemplo, para uma matriz de 500 elementos e uma grade de 250 threads, a thread com índice 20 na grade seria:\n",
    "\n",
    "- Realize sua operação no elemento 20 do array de 500 elementos\n",
    "- Incrementar seu índice em 250, o tamanho da grade, resultando em 270\n",
    "- Realize sua operação no elemento 270 do array de 500 elementos\n",
    "- Incrementar seu índice em 250, o tamanho da grade, resultando em 520\n",
    "- Como 520 agora está fora do alcance do array, o encadeamento interromperá seu trabalho\n",
    "\n",
    "CUDA fornece uma variável especial que fornece o número de blocos em uma grade, `gridDim.x`. Calcular o número total de threads em uma grade é simplesmente o número de blocos em uma grade multiplicado pelo número de threads em cada bloco, `gridDim.x * blockDim.x`. Com isso em mente, aqui está um exemplo detalhado de um loop grid-stride dentro de um kernel:\n",
    "\n",
    "\n",
    "```cpp\n",
    "__global__ void kernel(int *a, int N) {\n",
    "  int indexWithinTheGrid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  int gridStride = gridDim.x * blockDim.x;\n",
    "  for (int i = indexWithinTheGrid; i < N; i += gridStride) {\n",
    "    // do work on a[i];\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1667310825043,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "4iMHr2Kvdv8C",
    "outputId": "1953aff7-b207-4c35-db47-9f2edf01c490"
   },
   "outputs": [],
   "source": [
    "%%writefile 07-grid-stride-double.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "void init(int *a, int N) {\n",
    "  int i;\n",
    "  for (i = 0; i < N; ++i) \n",
    "    a[i] = i;\n",
    "}\n",
    "\n",
    "__global__ void doubleElements(int *a, int N) {\n",
    "  /* Use a grid-stride loop so each thread does work\n",
    "   * on more than one element in the array.  */\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = gridDim.x * blockDim.x;\n",
    "\n",
    "  for (int i = idx; i < N; i += stride) \n",
    "    a[i] *= 2;\n",
    "}\n",
    "\n",
    "bool checkElementsAreDoubled(int *a, int N) {\n",
    "  int i;\n",
    "  for (i = 0; i < N; ++i) \n",
    "    if (a[i] != i*2) \n",
    "      return false;\n",
    "  return true;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int N = 10000;\n",
    "  int *a;\n",
    "\n",
    "  size_t size = N * sizeof(int);\n",
    "  cudaMallocManaged(&a, size);\n",
    "\n",
    "  init(a, N);\n",
    "\n",
    "  size_t threads_per_block = 256;\n",
    "  size_t number_of_blocks = 32;\n",
    "\n",
    "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
    "  cudaDeviceSynchronize();\n",
    "\n",
    "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
    "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
    "\n",
    "  cudaFree(a);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2476,
     "status": "ok",
     "timestamp": 1667310854061,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "E54xcvLxeDoZ",
    "outputId": "d684c772-c4d2-432a-c6c4-fade4b161f30"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o grid-stride-double 07-grid-stride-double.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gN2npqAetcR"
   },
   "source": [
    "---\n",
    "## Tratamento de erros\n",
    "\n",
    "Como em qualquer aplicativo, o tratamento de erros no código CUDA acelerado é essencial. Muitas, se não a maioria das funções CUDA (veja, por exemplo, as [funções de gerenciamento de memória](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY)) retornam um valor do tipo `cudaError_t`, que pode ser usado para verificar se ocorreu ou não um erro ao chamar a função. Aqui está um exemplo onde o tratamento de erros é executado para uma chamada para `cudaMallocManaged`:\n",
    "\n",
    "```cpp\n",
    "cudaError_t err;\n",
    "err = cudaMallocManaged(&a, N) // Assumindo a existência de `a` e `N`.\n",
    "\n",
    "if (err != cudaSuccess) { // `cudaSuccess` é fornecido pelo CUDA.\n",
    "  printf(\"Erro: %s\\n\", cudaGetErrorString(err)); // `cudaGetErrorString` é fornecido pelo CUDA.\n",
    "}\n",
    "```\n",
    "\n",
    "O lançamento de kernels, que são definidos para retornar `void`, não retorna um valor do tipo `cudaError_t`. Para verificar os erros que ocorrem no momento da inicialização do kernel, por exemplo, se a configuração de inicialização estiver incorreta, o CUDA fornece a função `cudaGetLastError`, que retorna um valor do tipo `cudaError_t`.\n",
    "\n",
    "```cpp\n",
    "/* Este lançamento deve causar um erro, mas o próprio kernel não pode devolver. */\n",
    "algumKernel<<<1, -1>>>(); // -1 não é um número válido de threads.\n",
    "\n",
    "cudaError_t err;\n",
    "err = cudaGetLastError(); // `cudaGetLastError` retornará o erro acima.\n",
    "if (err!= cudaSucesso) {\n",
    "  printf(\"Erro: %s\\n\", cudaGetErrorString(err));\n",
    "}\n",
    "```\n",
    "\n",
    "Finalmente, para capturar erros que ocorrem de forma assíncrona, por exemplo, durante a execução de um kernel assíncrono, é essencial verificar o status retornado por uma chamada de API de tempo de execução CUDA de sincronização subsequente, como `cudaDeviceSynchronize`, que retornará um erro se um dos kernels lançados anteriormente falhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1667311379791,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "v6a6qaxOfrZP",
    "outputId": "dc54fd6b-14a1-473c-a9c2-d7083de99080"
   },
   "outputs": [],
   "source": [
    "%%writefile 08-add-error-handling.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "void init(int *a, int N) {\n",
    "  int i;\n",
    "  for (i = 0; i < N; ++i)\n",
    "    a[i] = i;\n",
    "}\n",
    "\n",
    "__global__ void doubleElements(int *a, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int stride = gridDim.x * blockDim.x;\n",
    "\n",
    "  /* The previous code (now commented out) attempted\n",
    "   * to access an element outside the range of `a`.  */\n",
    "\n",
    "  // for (int i = idx; i < N + stride; i += stride)\n",
    "  for (int i = idx; i < N; i += stride)\n",
    "    a[i] *= 2;\n",
    "}\n",
    "\n",
    "bool checkElementsAreDoubled(int *a, int N) {\n",
    "  int i;\n",
    "  for (i = 0; i < N; ++i)\n",
    "    if (a[i] != i*2) \n",
    "      return false;\n",
    "  return true;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  int N = 10000;\n",
    "  int *a;\n",
    "\n",
    "  size_t size = N * sizeof(int);\n",
    "  cudaMallocManaged(&a, size);\n",
    "\n",
    "  init(a, N);\n",
    "\n",
    "  /* The previous code (now commented out) attempted to launch\n",
    "   * the kernel with more than the maximum number of threads per\n",
    "   * block, which is 1024. */\n",
    "  size_t threads_per_block = 1024;\n",
    "  /* size_t threads_per_block = 2048; */\n",
    "  size_t number_of_blocks = 32;\n",
    "\n",
    "  cudaError_t syncErr, asyncErr;\n",
    "\n",
    "  doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);\n",
    "\n",
    "  /* Catch errors for both the kernel launch above and any\n",
    "   * errors that occur during the asynchronous `doubleElements`\n",
    "   * kernel execution. */\n",
    "\n",
    "  syncErr = cudaGetLastError();\n",
    "  asyncErr = cudaDeviceSynchronize();\n",
    "\n",
    "  /* Print errors should they exist. */\n",
    "\n",
    "  if (syncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(syncErr));\n",
    "  if (asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
    "\n",
    "  bool areDoubled = checkElementsAreDoubled(a, N);\n",
    "  printf(\"All elements were doubled? %s\\n\", areDoubled ? \"TRUE\" : \"FALSE\");\n",
    "\n",
    "  cudaFree(a);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1728,
     "status": "ok",
     "timestamp": 1667311416535,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "xWsKz0tigKkF",
    "outputId": "29b8ee2d-5e27-47bb-e281-a7951f409452"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o add-error-handling 08-add-error-handling.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kItQSgU4gYls"
   },
   "source": [
    "---\n",
    "## Resumo\n",
    "\n",
    "Neste momento, você atingiu todos os seguintes objetivos de laboratório:\n",
    "\n",
    "- Escrever, compilar e executar programas C/C++ que chamam funções de CPU e **lançament\n",
    "o** de **kernels** de GPU.\n",
    "- Controlar a **hierarquia de threads** paralela usando a **configuração de execução**.\n",
    "- Refatorar loops seriais para executar suas iterações em paralelo em uma GPU.\n",
    "- Alocar e liberar memória disponível para CPUs e GPUs.\n",
    "- Manipular erros gerados pelo código CUDA.\n",
    "\n",
    "Agora você concluirá o objetivo final do laboratório:\n",
    "\n",
    "- Acelerar aplicativos somente de CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N1D207PlMym"
   },
   "source": [
    "***\n",
    "## Sua vez! \n",
    "\n",
    "O código serial a seguir realiza a soma de dois vetores. Tente modificar o código com o que você aprendeu até agora para torná-lo paralelo. \n",
    "\n",
    "Para facilitar, eu coloquei na célula seguinte as mudanças necessárias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1667312847290,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "7K3bRuQmlizD",
    "outputId": "9396a87e-db38-4352-f91b-324dc6fe24f0"
   },
   "outputs": [],
   "source": [
    "%%writefile  09-vector-add.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "void initWith(float num, float *a, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; ++i)\n",
    "  {\n",
    "    a[i] = num;\n",
    "  }\n",
    "}\n",
    "\n",
    "void addVectorsInto(float *result, float *a, float *b, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; ++i)\n",
    "  {\n",
    "    result[i] = a[i] + b[i];\n",
    "  }\n",
    "}\n",
    "\n",
    "void checkElementsAre(float target, float *array, int N)\n",
    "{\n",
    "  for(int i = 0; i < N; i++)\n",
    "  {\n",
    "    if(array[i] != target)\n",
    "    {\n",
    "      printf(\"FAIL: array[%d] - %0.0f does not equal %0.0f\\n\", i, array[i], target);\n",
    "      exit(1);\n",
    "    }\n",
    "  }\n",
    "  printf(\"SUCCESS! All values added correctly.\\n\");\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  const int N = 2<<20;\n",
    "  size_t size = N * sizeof(float);\n",
    "\n",
    "  float *a;\n",
    "  float *b;\n",
    "  float *c;\n",
    "\n",
    "  a = (float *)malloc(size);\n",
    "  b = (float *)malloc(size);\n",
    "  c = (float *)malloc(size);\n",
    "\n",
    "  initWith(3, a, N);\n",
    "  initWith(4, b, N);\n",
    "  initWith(0, c, N);\n",
    "\n",
    "  addVectorsInto(c, a, b, N);\n",
    "\n",
    "  checkElementsAre(7, c, N);\n",
    "\n",
    "  free(a);\n",
    "  free(b);\n",
    "  free(c);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_70 -o vector-add 09-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYkmlJPqieUf"
   },
   "source": [
    "```diff\n",
    "1a2,11\n",
    "+ #include <assert.h>\n",
    "+ \n",
    "+ inline cudaError_t checkCuda(cudaError_t result)\n",
    "+ {\n",
    "+   if (result != cudaSuccess) {\n",
    "+     fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
    "+     assert(result == cudaSuccess);\n",
    "+   }\n",
    "+   return result;\n",
    "+ }\n",
    "10a21\n",
    "+ __global__\n",
    "13c24,27\n",
    "-   for(int i = 0; i - N; ++i)\n",
    "---\n",
    "+   int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "+   int stride = blockDim.x * gridDim.x;\n",
    "+ \n",
    "+   for(int i = index; i - N; i += stride)\n",
    "41,43c55,57\n",
    "-   a = (float *)malloc(size);\n",
    "-   b = (float *)malloc(size);\n",
    "-   c = (float *)malloc(size);\n",
    "---\n",
    "+   checkCuda( cudaMallocManaged(&a, size) );\n",
    "+   checkCuda( cudaMallocManaged(&b, size) );\n",
    "+   checkCuda( cudaMallocManaged(&c, size) );\n",
    "49c63,72\n",
    "-   addVectorsInto(c, a, b, N);\n",
    "---\n",
    "+   size_t threadsPerBlock;\n",
    "+   size_t numberOfBlocks;\n",
    "+ \n",
    "+   threadsPerBlock = 256;\n",
    "+   numberOfBlocks = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
    "+ \n",
    "+   addVectorsInto<<<numberOfBlocks, threadsPerBlock>>>(c, a, b, N);\n",
    "+ \n",
    "+   checkCuda( cudaGetLastError() );\n",
    "+   checkCuda( cudaDeviceSynchronize() );\n",
    "53,55c76,78\n",
    "-   free(a);\n",
    "-   free(b);\n",
    "-   free(c);\n",
    "---\n",
    "+   checkCuda( cudaFree(a) );\n",
    "+   checkCuda( cudaFree(b) );\n",
    "+   checkCuda( cudaFree(c) );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnHPXbHmf4Pn"
   },
   "source": [
    "___\n",
    "# CUDA C/C++ vs. Numba vs. pyCUDA\n",
    "\n",
    "A maneira mais comum de programar em CUDA é com as extensões de linguagem CUDA C/C++. Com relação ao Python, o pyCUDA é, além do Numba, uma alternativa para acelerar o código Python em GPUs. Vale a pena fazer uma rápida comparação das três opções mencionadas antes de começarmos:\n",
    "\n",
    "### CUDA C/C++:\n",
    "\n",
    "- A maneira mais comum, eficiente e flexível de utilizar CUDA\n",
    "- Acelera aplicações C/C++\n",
    "\n",
    "### pyCUDA:\n",
    "\n",
    "- Expõe toda a API CUDA C/C++\n",
    "- É a opção CUDA com melhor desempenho disponível para Python\n",
    "- Requer escrever código C em seu Python e, em geral, muitas modificações de código\n",
    "\n",
    "### Numba:\n",
    "\n",
    "- Potencialmente menos desempenho que pyCUDA\n",
    "- Não expõe (ainda?) toda a API CUDA C/C++\n",
    "- Ainda permite aceleração massiva, geralmente com muito pouca modificação de código\n",
    "- Permite aos desenvolvedores a conveniência de escrever código diretamente em Python\n",
    "- Também otimiza o código Python para a CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiros passos: Compilação para CPU\n",
    "Se você se lembra, o Numba pode ser usado para otimizar o código para uma CPU ou GPU. Como introdução, e antes de passar para a aceleração de GPU, vamos escrever nossa primeira função Numba e compilá-la para o **CPU**. Ao fazer isso, teremos uma entrada fácil na sintaxe do Numba e também teremos a oportunidade um pouco mais tarde de comparar o desempenho do código Numba otimizado por CPU com o código Numba acelerado por GPU.\n",
    "\n",
    "O compilador Numba normalmente é ativado aplicando um [**function decorator**](https://en.wikipedia.org/wiki/Python_syntax_and_semantics#Decorators) a uma função Python. Decoradores são modificadores de função que transformam as funções Python que eles decoram, usando uma sintaxe muito simples. Aqui usaremos o decorador de compilação da CPU do Numba `@jit`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1667312825454,
     "user": {
      "displayName": "Ricardo Menotti - UFSCar",
      "userId": "06481705935960912252"
     },
     "user_tz": 180
    },
    "id": "JKFNQNhWglH7",
    "outputId": "55c680c6-e656-4e22-e9e6-6e2b29581ec6"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import math\n",
    "\n",
    "# This is the function decorator syntax and is equivalent to `hypot = jit(hypot)`.\n",
    "# The Numba compiler is just a function you can call whenever you want!\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)\n",
    "\n",
    "hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraremos em mais detalhes abaixo sobre o que acontece quando `hypot` é chamado, mas por enquanto saiba que na primeira vez que chamamos `hypot`, o compilador é acionado e compila uma implementação de código de máquina da função para entradas float. O Numba também salva a implementação original do Python da função no atributo `.py_func`, para que possamos chamar o código original do Python para ter certeza de obter a mesma resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BenchMark\n",
    "\n",
    "Uma parte importante do uso do Numba é medir o desempenho do seu novo código. Vamos ver se realmente aceleramos alguma coisa. A maneira mais fácil de fazer isso em um notebook Jupyter, como aquele em que esta sessão é executada, é usar a [`%timeit` função mágica](https://ipython.readthedocs.io/en/stable/interactive/magics .html#magic-timeit). Vamos primeiro medir a velocidade do Python original:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit math.hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O built-in do Python é ainda mais rápido que o Numba! Isso ocorre porque o Numba introduz alguma sobrecarga para cada chamada de função que é maior que a sobrecarga de chamada de função do próprio Python. Funções extremamente rápidas (como a acima) serão prejudicadas por isso. (Como um aparte, se você chamar uma função Numba de outra, haverá muito pouca sobrecarga de função, às vezes até zero se o compilador inserir a função na outra. Em resumo, sempre faça benchmark de suas funções para obter evidências de aceleração.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício: Uso do Numba para compilação de uma função para CPU\n",
    "\n",
    "A função a seguir usa [o método de Monte Carlo para determinar o Pi](https://academo.org/demos/estimating-pi-monte-carlo/). \n",
    "A função em si já está funcionando, então não se preocupe com os detalhes matemáticos da implementação.\n",
    "\n",
    "Complete as tasks para compilar `monte_carlo_pi` com Numba antes de executar as 3 células a seguir que irão:\n",
    "\n",
    "  1. Confirme se a versão compilada está se comportando da mesma forma que a versão não compilada.\n",
    "  2. Faça um benchmark da versão não compilada.\n",
    "  3. Faça um benchmark da versão compilada.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import Numba's just-in-time compiler function\n",
    "from numba import jit\n",
    "import random\n",
    "\n",
    "# We will use numpy's `testing` library to confirm compiled and uncompiled versions run the same\n",
    "from numpy import testing\n",
    "\n",
    "nsamples = 1000000\n",
    "\n",
    "\n",
    "# TODO: Use the Numba compiler to compile this function\n",
    "@jit\n",
    "def monte_carlo_pi(nsamples):\n",
    "    acc = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x**2 + y**2) < 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / nsamples\n",
    "\n",
    "# This assertion will fail until you successfully complete the exercise one cell above\n",
    "testing.assert_almost_equal(monte_carlo_pi(nsamples), monte_carlo_pi.py_func(nsamples), decimal=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit monte_carlo_pi(nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit monte_carlo_pi.py_func(nsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamento do Numba\n",
    "\n",
    "Agora que tivemos o primeiro contato usando o compilador Numba, vamos dar um olhada no que realmente está ocorrendo por baixo dos panos. A primeira vez que chamamos nossa função `hypot` encapsulada em Numba, o seguinte processo foi iniciado:\n",
    "\n",
    "![Numba Flowchart](images/numba_flowchart.png \"The compilation process\")\n",
    "\n",
    "\n",
    "Podemos ver o resultado da inferência de tipo usando o método `.inspect_types()`, que imprime uma versão anotada do código-fonte:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypot.inspect_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os nomes de tipo de Numba tendem a se assemelhar aos [tipos de dados do Numpy](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html), então um Python `float` é um `float64` (também chamado de \"precisão dupla\" em outras linguagens). Observar os tipos de dados às vezes pode ser importante no código da GPU porque o desempenho dos cálculos `float32` e `float64` pode (dependendo da GPU) ser muito diferente em dispositivos CUDA. Se o seu algoritmo pode obter resultados corretos usando `float32`, então você provavelmente deve usar esse tipo de dados, porque a conversão para `float64` pode, dependendo do tipo de GPU, diminuir drasticamente a função.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objeto e Modo \"nopython\" \n",
    "\n",
    "Numba não pode compilar todo o código Python. Algumas funções não têm uma tradução Numba, e alguns tipos de tipos Python não podem ser compilados de forma eficiente (ainda). Por exemplo, Numba não suporta dicionários (no momento da redação deste artigo). Por conta disso, o Numba retornará um modo chamado **modo de objeto**, que não faz especialização de tipo. O modo de objeto existe para habilitar outras funcionalidades do Numba, mas em muitos casos, você deseja que o Numba informe se a inferência de tipo falhar. Você pode forçar o **nopython mode** (o outro modo de compilação).\n",
    "Aqui vamos tentar compilar algum código Python que o Numba ainda não sabe compilar, passando o argumento `nopython` para o decorador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def cannot_compile(x):\n",
    "    return x['key']\n",
    "\n",
    "cannot_compile(dict(key='value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, obtemos uma exceção quando o Numba tenta compilar a função e, se você rolar para baixo até o final da saída da exceção, verá um erro que descreve o erro da célula acima:\n",
    "```\n",
    "- argument 0: cannot determine Numba type of <class 'dict'>\n",
    "```\n",
    "**Usar o modo `nopython` é a maneira recomendada e prática recomendada de usar `jit`, pois leva ao melhor desempenho.**\n",
    "\n",
    "Numba fornece outro decorador `njit` que é um alias para `jit(nopython=True)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def cannot_compile(x):\n",
    "    return x['key']\n",
    "    \n",
    "cannot_compile(dict(key='value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse a [documentação Numba](https://numba.pydata.org/numba-doc/dev/reference/pysupported.html), para maiores informações sobre funções suportadas em Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao Numba para a GPU com NumPy Universal Functions (ufuncs)\n",
    "\n",
    "Começaremos nossa cobertura da programação de GPU no Numba com como compilar [Funções universais NumPy \\(ou ufuncs\\)](https://docs.scipy.org/doc/numpy-1.15.1/reference/ufuncs.html) para a GPU.\n",
    "\n",
    "A coisa mais importante a saber sobre a programação de GPU quando começamos é que o hardware da GPU é projetado para *paralelismo de dados*. A taxa de transferência máxima é alcançada quando a GPU está computando as mesmas operações em muitos elementos diferentes ao mesmo tempo.\n",
    "\n",
    "As funções universais do NumPy, que executam a mesma operação em todos os elementos em uma matriz NumPy, são naturalmente paralelas aos dados, portanto, são um ajuste natural para a programação da GPU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo ufuncs para a GPU\n",
    "\n",
    "O Numba tem a capacidade de criar ufuncs *compilados*, normalmente um processo não tão simples envolvendo código C. Com o Numba você simplesmente implementa uma função escalar a ser executada em todas as entradas, decora-a com `@vectorize`, e o Numba descobrirá as regras de transmissão para você. Para aqueles que estão familiarizados com o `vectorize` do NumPy (https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.vectorize.html), o decorador `vectorize` do Numba será muito familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso primeiro exemplo iremos usar o `@vectorize` decorador para compilar e otimizar a ufunc para **CPU**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import numpy as np \n",
    "\n",
    "@vectorize\n",
    "def add_ten(num):\n",
    "    return num + 10 # This scalar operation will be performed on each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.arange(10)\n",
    "add_ten(nums) # pass the whole array into the ufunc, it performs the operation on each element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos gerando um ufunc que usa CUDA na GPU com a adição de fornecer uma **assinatura de tipo explícita** e definir o atributo `target`. O argumento de assinatura de tipo descreve quais tipos usar tanto para os argumentos ufuncs quanto para o valor de retorno:\n",
    "``` python\n",
    "'return_value_type(argument1_value_type, argument2_value_type, ...)'\n",
    "```\n",
    "\n",
    "Consulte os documentos do Numba para obter mais informações sobre [tipos disponíveis](https://numba.pydata.org/numba-doc/dev/reference/types.html), bem como para obter informações adicionais sobre [escrever ufuncs com mais de um assinatura](https://numba.pydata.org/numba-doc/dev/user/vectorize.html)\n",
    "\n",
    "Aqui está um exemplo simples de um ufunc que será compilado para um dispositivo GPU habilitado para CUDA. Ele espera dois valores `int64` e retorna também um valor `int64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['int64(int64, int64)'], target='cuda') # Type signature and target are required for the GPU\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([10, 20, 30, 40])\n",
    "\n",
    "add_ufunc(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma chamada de função tão simples, muitas coisas simplesmente aconteceram! Numba apenas automaticamente:\n",
    "\n",
    " * Compilou um kernel CUDA para executar a operação ufunc em paralelo sobre todos os elementos de entrada.\n",
    " * Memória GPU alocada para as entradas e saídas.\n",
    " * Copiou os dados de entrada para a GPU.\n",
    " * Executou o kernel CUDA (função GPU) com as dimensões corretas do kernel, considerando os tamanhos de entrada.\n",
    " * Copiou o resultado de volta da GPU para a CPU.\n",
    " * Devolveu o resultado como um array NumPy no host.\n",
    " \n",
    "Comparado a uma implementação em C, o acima é notavelmente mais conciso.\n",
    "\n",
    "Você pode estar se perguntando o quão rápido nosso exemplo simples está na GPU? Vamos ver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.add(b, c)   # NumPy on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_ufunc(b, c) # Numba on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, a GPU é *muito mais lenta* que a CPU?? Por enquanto, isso é esperado porque usamos (deliberadamente) a GPU de várias maneiras neste exemplo. Como usamos mal a GPU ajudará a esclarecer quais tipos de problemas são adequados para a computação da GPU e quais são melhores para serem executados na CPU:\n",
    "\n",
    "  * **Nossas entradas são muito pequenas**: a GPU obtém desempenho por meio do paralelismo, operando em milhares de valores de uma só vez. Nossas entradas de teste têm apenas 4 e 16 inteiros, respectivamente. Precisamos de uma matriz muito maior para manter a GPU ocupada.\n",
    "  * **Nosso cálculo é muito simples**: enviar um cálculo para a GPU envolve um pouco de sobrecarga em comparação com a chamada de uma função na CPU. Se nosso cálculo não envolver operações matemáticas suficientes (geralmente chamadas de \"intensidade aritmética\"), a GPU passará a maior parte do tempo esperando que os dados se movam.\n",
    "  * **Copiamos os dados da CPU para a GPU**: embora em alguns cenários, pagar o custo de copiar dados da CPU para a GPU possa valer a pena para uma única função, geralmente será preferível executar várias operações GPUs em sequência. Nesses casos, faz sentido enviar dados para a GPU e mantê-los lá até que todo o nosso processamento seja concluído.\n",
    "  * **Nossos tipos de dados são maiores que o necessário**: Nosso exemplo usa `int64` quando provavelmente não precisamos dele. O código escalar usando tipos de dados de 32 e 64 bits executa basicamente a mesma velocidade na CPU e, para tipos inteiros, a diferença pode não ser drástica, mas os tipos de dados de ponto flutuante de 64 bits podem ter um custo de desempenho significativo na GPU, dependendo do tipo de GPU. A aritmética básica em floats de 64 bits pode ser de 2x (Pascal-architecture Tesla) a 24x (Maxwell-architecture GeForce) mais lenta do que floats de 32 bits. Se você estiver usando GPUs mais modernas (Volta, Turing, Ampere), isso pode ser muito menos preocupante. O NumPy é padronizado para tipos de dados de 64 bits ao criar matrizes, por isso é importante definir o [`dtype`](https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.dtypes.html ) ou use o método [`ndarray.astype()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.astype.html) para escolher o tipo de 32 bits quando você precisar deles.\n",
    "  \n",
    "  \n",
    "Dado o exposto, vamos tentar um exemplo que é mais rápido na GPU realizando uma operação com intensidade aritmética muito maior, em uma entrada muito maior e usando um tipo de dados de 32 bits.\n",
    "\n",
    "**Observação:** Nem todo código NumPy funcionará na GPU e, como no exemplo a seguir, precisaremos usar o `pi` e o `exp` da biblioteca `math` em vez do NumPy. Consulte [os documentos do Numba](https://numba.pydata.org/numba-doc/latest/reference/numpysupported.html) para obter uma ampla cobertura do suporte ao NumPy na GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # Note that for the CUDA target, we need to use the scalar functions from the math module, not NumPy\n",
    "\n",
    "SQRT_2PI = np.float32((2*math.pi)**0.5)  # Precompute this constant as a float32.  Numba will inline it at compile time.\n",
    "\n",
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def gaussian_pdf(x, mean, sigma):\n",
    "    '''Compute the value of a Gaussian probability density function at x with given mean and sigma.'''\n",
    "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Evaluate the Gaussian a million times!\n",
    "x = np.random.uniform(-3, 3, size=1000000).astype(np.float32)\n",
    "mean = np.float32(0.0)\n",
    "sigma = np.float32(1.0)\n",
    "\n",
    "# Quick test on a single element just to make sure it works\n",
    "gaussian_pdf(x[0], 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats # for definition of gaussian distribution, so we can compare CPU to GPU time\n",
    "norm_pdf = scipy.stats.norm\n",
    "%timeit norm_pdf.pdf(x, loc=mean, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gaussian_pdf(x, mean, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Device Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allowed Python on the GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: GPU Accelerate a Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTce41AoYIqp"
   },
   "source": [
    "## Referências\n",
    "\n",
    "- https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/course/\n",
    "- https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/course/\n",
    "- https://developer.nvidia.com/blog/how-query-device-properties-and-handle-errors-cuda-cc/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO3iuMtqTQyr/GMWeimjFKK",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7fb4e08d40c87d182d3eb80acd71ade9835941d2f88b8731e54a532e624040ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
